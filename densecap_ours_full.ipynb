{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import shlex, subprocess\n",
    "sys.path.append('/workspace/dense_cap')\n",
    "sys.path.append('/workspace/dense_cap/densecap')\n",
    "sys.path.append('/workspace/dense_cap/densecap/model')\n",
    "sys.path.append('/workspace/dense_cap/tools/densevid_eval/coco-caption')\n",
    "from tools.eval_proposal_anet import ANETproposal\n",
    "from transformer_ori import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import math\n",
    "from data.utils import segment_iou\n",
    "import time\n",
    "import os\n",
    "import easydict\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from torch.cuda.amp import autocast,GradScaler\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from data.anet_dataset_our_matching_deepct import ANetDataset, anet_collate_fn, get_vocab_and_sentences\n",
    "from data.anet_test_dataset_our_matching_deepct import ANetTestDataset, anet_test_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "os.environ[\"PATH\"] = '/opt/conda/envs/py37/bin:/opt/conda/condabin:/opt/conda/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/jdk-14.0.2/bin'\n",
    "d_model =1024\n",
    "d_hidden = d_model*2\n",
    "n_heads = 8\n",
    "batch_size =12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_name = \"dual_encoder_final\"\n",
    "model_name = evidence_name + '_final'\n",
    "start_from = \"../checkpoint/{}/model_caption_best.t7\".format(model_name)\n",
    "start_from = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weight = 0.1\n",
    "temporal_weight = 0.1\n",
    "multitask_weight = 0.1\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    'cfgs_file':'cfgs/our_yc2_only.yml', 'dataset':'our_yc2_only', 'dataset_file':'/workspace/dense_cap/densecap/data/our/our_yc2_only_annotations_new_trainval.json',\n",
    "    'feature_root': \"/workspace/dense_cap/our_dataset\",'dur_file': \"/workspace/dense_cap/densecap/data/our/our_yc2_only_duration_frame.csv\", #'dist_url': \"file:///workdir/mixiaoyue/nonexistent_file\",\n",
    "    'train_data_folder':['new_training'], 'val_data_folder': ['new_validation'],'test_data_folder':['new_testing'],'num_workers':0,\n",
    "    'max_sentence_len':20, 'd_model':d_model, 'd_hidden': d_hidden, 'n_heads':n_heads, 'in_emb_dropout':0.1,\n",
    "    'attn_dropout':0.2, 'vis_emb_dropout':0.1, 'cap_dropout':0.2, 'image_feat_size':3072, 'min_prop_num':50, 'max_prop_num':500,'min_prop_before_nms':200,\n",
    "    'n_layers':2, 'train_sample':20, 'sample_prob':0, 'slide_window_size':560, 'slide_window_stride':20,\n",
    "    'sampling_sec':1.5, 'kernel_list':[1, 2, 3, 4, 5, 7, 9, 11, 15, 21, 29, 41, 57, 71, 111, 161, 211, 251],\n",
    "    'pos_thresh':0.7,'neg_thresh':0.3,'stride_factor':50, 'max_epochs':31,'batch_size':batch_size,'valid_batch_size':int(batch_size*1.5),\n",
    "    'cls_weight':0.1, 'reg_weight':10, 'sent_weight':1.0, 'scst_weight':0.0, 'mask_weight':0.75, 'id':'{}'.format(model_name), 'optim':'adam',\n",
    "    'learning_rate':learning_rate, 'alpha':0.95, 'beta':0.999, 'epsilon':1e-8, 'loss_alpha_r':2, 'patience_epoch':2, 'reduce_factor':0.9, \n",
    "    'grad_norm':1,'dist_backend':'gloo', 'world_size':1,'using_dual': False,'save_checkpoint_every':1,'checkpoint_path':'/workspace/dense_cap/checkpoint/models/lxmert_deepct',\n",
    "    'losses_log_every':1,'seed':213,'cuda':'cuda','learn_mask':True,\n",
    "    'save_train_samplelist':'store_true', 'load_train_samplelist': True, 'save_valid_samplelist':'store_true', 'load_valid_samplelist': True,\n",
    "    'save_test_samplelist':'store_true', 'load_test_samplelist': True,'test_samplelist_path':'/workspace/dense_cap/samplelist/new_test_samplelist.pkl',\n",
    "    'train_samplelist_path':'/workspace/dense_cap/samplelist/new_train_samplelist.pkl','valid_samplelist_path':'/workspace/dense_cap/samplelist/new_valid_samplelist.pkl',\n",
    "    'test_raw_data':'/workspace/dense_cap/densecap/data/our/our_yc2_only_annotations_new_test.json',\n",
    "    'start_from': start_from, 'distributed':False, 'using_recipe':True, 'using_dual':False, 'gated_mask':True, 'enable_visdom':False,\n",
    "    'before':False, 'densecap_references': [\"/workspace/dense_cap/densecap/data/our/new_test_our_yc2_only.json\"],\n",
    "    'densecap_eval_file': '/workspace/dense_cap/densecap/tools/densevid_eval/evaluate.py', 'epoch':50, 'query_transform_dim':d_model,\n",
    "    'v_e_temporal':True,'v_v_contrast':False, 'v_e_contrast':True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_file:  /workspace/dense_cap/densecap/data/our/our_yc2_only_annotations_new_trainval.json\n",
      "# of words in the vocab: 4\n",
      "# of sentences in training: 0, # of sentences in validation: 0\n",
      "# of training videos: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('original_num_frame.json', 'r') as fp:\n",
    "    of = json.load( fp)\n",
    "    import pickle\n",
    "newof = {}\n",
    "for video_prefix, val in of.items():\n",
    "    video_prefix = video_prefix.split('/')\n",
    "    video_prefix[4] = 'new_testing'\n",
    "    newof['/'.join(video_prefix)] = val\n",
    "with open('duration_frame.pkl', 'rb') as fp:\n",
    "    duration_frame = pickle.load( fp)\n",
    "from scripts.train_our_copynet_adam_deepct import get_vocab_and_sentences\n",
    "text_proc, raw_data = get_vocab_and_sentences(args.dataset_file, args.max_sentence_len)\n",
    "gold_seg = {}\n",
    "sampling_sec = 1.5\n",
    "for vid, val in raw_data.items():\n",
    "    gold_seg[vid] = {(int(item['segment'][0]/1.5),int(item['segment'][1]/1.5)):item['sentence']  for item in val['annotations']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "recipe_feat_path = os.path.join('/workspace', 'dense_cap', 'our_dataset', 'recipe_feat.pkl')\n",
    "with open(recipe_feat_path, 'rb') as fp:\n",
    "    recipe_feat = pkl.load(fp)\n",
    "    \n",
    "match_dict_path = os.path.join('/workspace/evidence_retrieval', evidence_name + '_matched_result.pkl')\n",
    "with open(match_dict_path, 'rb') as fp:\n",
    "    match_dict = pkl.load(fp)\n",
    "def get_nearest_frame_idx(frame_idx, frame_indices):\n",
    "    frame_indices = np.array(frame_indices)\n",
    "    recipe_frame_index = np.argmin(np.abs(frame_indices-frame_idx))\n",
    "    return frame_indices[recipe_frame_index]\n",
    "def get_matched_recipe_feat(vids, duration_frame):\n",
    "    batch_recipe_feat = []\n",
    "    for b, vid in enumerate(vids):\n",
    "        vid_match = match_dict[vid]\n",
    "        vid_recipe_key = list(vid_match.keys())\n",
    "        vid_recipe_key.sort()\n",
    "        vid_recipe_feat = []\n",
    "        for i in range(560):\n",
    "            if i <duration_frame[vid]:\n",
    "                matched_recipe_index = get_nearest_frame_idx(i, vid_recipe_key)\n",
    "                if matched_recipe_index not in vid_match.keys():\n",
    "                    matched_recipe_index = max(vid_match.keys())\n",
    "                frame_recipe_feat = recipe_feat['split_ins'][vid_match[matched_recipe_index]]\n",
    "            else:\n",
    "                frame_recipe_feat = torch.zeros_like(frame_recipe_feat)\n",
    "            vid_recipe_feat.append(frame_recipe_feat)\n",
    "        vid_recipe_feat = torch.cat(vid_recipe_feat).unsqueeze(0)\n",
    "        batch_recipe_feat.append(vid_recipe_feat)\n",
    "    batch_recipe_feat = torch.cat(batch_recipe_feat)\n",
    "    return batch_recipe_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/evidence_retrieval/dual_encoder_final_matched_result.pkl'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_dict_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encodings(x, D):\n",
    "    # input x a vector of positions\n",
    "    encodings = torch.zeros(x.size(0), D)\n",
    "    if x.is_cuda:\n",
    "        encodings = encodings.cuda(x.get_device())\n",
    "    encodings = Variable(encodings)\n",
    "\n",
    "    for channel in range(D):\n",
    "        if channel % 2 == 0:\n",
    "            encodings[:,channel] = torch.sin(\n",
    "                x / 10000 ** (channel / D))\n",
    "        else:\n",
    "            encodings[:,channel] = torch.cos(\n",
    "                x / 10000 ** ((channel - 1) / D))\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DropoutTime1D(nn.Module):\n",
    "    '''\n",
    "        assumes the first dimension is batch, \n",
    "        input in shape B x T x H\n",
    "        '''\n",
    "    def __init__(self, p_drop):\n",
    "        super(DropoutTime1D, self).__init__()\n",
    "        self.p_drop = p_drop\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            mask = x.data.new(x.data.size(0),x.data.size(1), 1).uniform_()\n",
    "            mask = Variable((mask > self.p_drop).float())\n",
    "            \n",
    "            return x * mask\n",
    "        else:\n",
    "            return x * (1-self.p_drop)\n",
    "\n",
    "    def init_params(self):\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        repstr = self.__class__.__name__ + ' (\\n'\n",
    "        repstr += \"{:.2f}\".format(self.p_drop)\n",
    "        repstr += ')'\n",
    "        return repstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionPropDenseCap(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden, n_layers, n_heads, vocab,\n",
    "                 in_emb_dropout, attn_dropout, vis_emb_dropout,\n",
    "                 cap_dropout, nsamples, kernel_list, stride_factor,\n",
    "                 learn_mask=False, window_length=560):\n",
    "        super(ActionPropDenseCap, self).__init__()\n",
    "        print(\"voca len: \", len(vocab))\n",
    "        self.kernel_list = kernel_list\n",
    "        self.nsamples = nsamples\n",
    "        self.learn_mask = learn_mask\n",
    "        self.d_model = d_model\n",
    "        self.dim_embedder = nn.Linear(768*2,d_model)\n",
    "        self.mask_model = nn.Sequential(\n",
    "            nn.Linear(d_model+window_length, d_model, bias=False),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, window_length),\n",
    "            #nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        #self.query_embed = segment_query_embed(args)\n",
    "        self.vis_emb = Transformer(d_model, 0, 0,\n",
    "                                   d_hidden=d_hidden,\n",
    "                                   n_layers=n_layers,\n",
    "                                   n_heads=n_heads,\n",
    "                                   drop_ratio=attn_dropout)\n",
    "\n",
    "        self.vis_dropout = DropoutTime1D(vis_emb_dropout)\n",
    "        #args.query_transform_dim\n",
    "        self.prop_out = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                nn.BatchNorm1d(d_model),\n",
    "                nn.Conv1d(d_model, d_model,\n",
    "                          self.kernel_list[i],\n",
    "                          stride=math.ceil(kernel_list[i]/stride_factor),\n",
    "                          groups=d_model,\n",
    "                          bias=False),\n",
    "                nn.BatchNorm1d(d_model),\n",
    "                nn.Conv1d(d_model, d_model,\n",
    "                          1, bias=False),\n",
    "                nn.BatchNorm1d(d_model),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(d_model),\n",
    "                nn.Conv1d(d_model, 4,\n",
    "                          1)\n",
    "                )\n",
    "                for i in range(len(self.kernel_list))\n",
    "            ])#+args.query_transform_dim\n",
    "\n",
    "        #print(\"d_model: \", d_model)\n",
    "        self.cap_model = RealTransformer(d_model,\n",
    "                                         self.vis_emb.encoder, #share the encoder\n",
    "                                         vocab,\n",
    "                                         d_hidden=d_hidden,\n",
    "                                         n_layers=n_layers,\n",
    "                                         n_heads=n_heads,\n",
    "                                         drop_ratio=cap_dropout)\n",
    "\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.reg_loss = nn.SmoothL1Loss()\n",
    "        self.l2_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x, s_pos, s_neg, sentence,\n",
    "                sample_prob=0, stride_factor=10, scst=False,\n",
    "                gated_mask=False, using_recipe=False, recipe_vector = None, dual=False, nff = None,vids = None, gold_segs = None):\n",
    "        B, T, _ = x.size()\n",
    "        dtype = x.data.type()\n",
    "\n",
    "        #x_rgb, x_flow = torch.split(x, 2048, 2)\n",
    "        #x_rgb = self.rgb_emb(x_rgb.contiguous())\n",
    "        #x_flow = self.flow_emb(x_flow.contiguous())\n",
    "\n",
    "        \n",
    "        #x = torch.cat((x_rgb, x_flow), 2)\n",
    "        \n",
    "        #print(\"x: \", x.shape)\n",
    "        #print(\"after x: \", x.shape)\n",
    "        if using_recipe and recipe_vector is not None:\n",
    "            #query_with_attention, temporal_loss, attention_score  = self.query_embed(recipe_vector, _final_tensor = vis_feat,\n",
    "            #                                                                         vids= vids, gold_segs = gold_segs)\n",
    "            x = torch.cat([x, recipe_vector], dim = -1)\n",
    "        x = self.dim_embedder(x)\n",
    "        vis_feat, all_emb = self.vis_emb(x, query=False, dual=False)\n",
    "        #print('visfeat',vis_feat.shape)\n",
    "        \"\"\"if using_recipe and recipe_vector is not None:\n",
    "            query_with_attention, temporal_loss, attention_score  = self.query_embed(recipe_vector, _final_tensor = vis_feat,\n",
    "                                                                                     vids= vids, gold_segs = gold_segs)\n",
    "            vis_feat = torch.cat([vis_feat, query_with_attention], dim = -1)\"\"\"\n",
    "\n",
    "        # B x T x H -> B x H x T\n",
    "        # for 1d conv\n",
    "        vis_feat = vis_feat.transpose(1,2).contiguous()\n",
    "\n",
    "        prop_lst = []\n",
    "        for i, kernel in enumerate(self.prop_out):\n",
    "\n",
    "            kernel_size = self.kernel_list[i]\n",
    "            if kernel_size <= vis_feat.size(-1):\n",
    "                pred_o = kernel(vis_feat)\n",
    "                anchor_c = Variable(torch.FloatTensor(np.arange(\n",
    "                    float(kernel_size)/2.0,\n",
    "                    float(T+1-kernel_size/2.0),\n",
    "                    math.ceil(kernel_size/stride_factor)\n",
    "                )).type(dtype))\n",
    "                if anchor_c.size(0) != pred_o.size(-1):\n",
    "                    raise Exception(\"size mismatch!\")\n",
    "\n",
    "                anchor_c = anchor_c.expand(B, 1, anchor_c.size(0))\n",
    "                anchor_l = Variable(torch.FloatTensor(anchor_c.size()).fill_(kernel_size).type(dtype))\n",
    "\n",
    "                pred_final = torch.cat((pred_o, anchor_l, anchor_c), 1)\n",
    "                \n",
    "                prop_lst.append(pred_final)\n",
    "            else:\n",
    "                #print('skipping kernel sizes greater than {}'.format(self.kernel_list[i]))\n",
    "                break\n",
    "\n",
    "        # Important! In prop_all, for the first dimension, the four values are proposal score, overlapping score (DEPRECATED!), length offset, and center offset, respectively\n",
    "        prop_all = torch.cat(prop_lst, 2)\n",
    "        #print(\"prop_all: \", prop_all.shape)\n",
    "        if B != s_pos.size(0) or B != s_neg.size(0):\n",
    "            raise Exception('feature and ground-truth segment do not match!')\n",
    "\n",
    "        sample_each = self.nsamples // 2\n",
    "        pred_score = Variable(torch.FloatTensor(np.zeros((sample_each*B, 2))).type(dtype))\n",
    "        gt_score = Variable(torch.FloatTensor(np.zeros((sample_each*B, 2))).type(dtype))\n",
    "        pred_offsets = Variable(torch.FloatTensor(np.zeros((sample_each*B,2))).type(dtype))\n",
    "        gt_offsets = Variable(torch.FloatTensor(np.zeros((sample_each*B,2))).type(dtype))\n",
    "\n",
    "        # B x T x H\n",
    "        batch_mask = Variable(torch.FloatTensor(np.zeros((B,T,1))).type(dtype))\n",
    "\n",
    "        # store positional encodings, size of B x 4,\n",
    "        # the first B values are predicted starts,\n",
    "        # second B values are predicted ends,\n",
    "        # third B values are anchor starts,\n",
    "        # last B values are anchor ends\n",
    "        pe_locs = Variable(torch.zeros(B*4).type(dtype))\n",
    "        anchor_window_mask = Variable(torch.zeros(B, T).type(dtype),\n",
    "                                      requires_grad=False)\n",
    "        pred_bin_window_mask = Variable(torch.zeros(B, T).type(dtype),\n",
    "                                      requires_grad=False)\n",
    "        gate_scores = Variable(torch.zeros(B,1,1).type(dtype))\n",
    "\n",
    "        mask_loss = None\n",
    "\n",
    "        pred_len = prop_all[:, 4, :] * torch.exp(prop_all[:, 2, :])\n",
    "        pred_cen = prop_all[:, 5, :] + prop_all[:, 4, :] * prop_all[:, 3, :]\n",
    "\n",
    "        for b in range(B):\n",
    "            pos_anchor = s_pos[b]\n",
    "            neg_anchor = s_neg[b]\n",
    "            #print(\"pos_anchor size: \", pos_anchor.size(0))\n",
    "            #print('neg_anchor size: ', neg_anchor.size(0))\n",
    "\n",
    "            if pos_anchor.size(0) != sample_each or neg_anchor.size(0) != sample_each:\n",
    "                raise Exception(\"# of positive or negative samples does not match\")\n",
    "\n",
    "            # randomly choose one of the positive samples to caption\n",
    "            pred_index = np.random.randint(sample_each)\n",
    "\n",
    "            # random sample anchors from different length\n",
    "            for i in range(sample_each):\n",
    "                # sample pos anchors\n",
    "                pos_sam = pos_anchor[i].data\n",
    "                pos_sam_ind = int(pos_sam[0])\n",
    "\n",
    "                pred_score[b*sample_each+i, 0] = prop_all[b, 0, pos_sam_ind]\n",
    "                gt_score[b*sample_each+i, 0] = 1\n",
    "\n",
    "                pred_offsets[b*sample_each+i] = prop_all[b, 2:4, pos_sam_ind]\n",
    "                gt_offsets[b*sample_each+i] = pos_sam[2:]\n",
    "\n",
    "                # sample neg anchors\n",
    "                neg_sam = neg_anchor[i].data\n",
    "                neg_sam_ind = int(neg_sam[0])\n",
    "                pred_score[b*sample_each+i, 1] = prop_all[b, 0, neg_sam_ind]\n",
    "                gt_score[b*sample_each+i, 1] = 0\n",
    "\n",
    "                # caption the segment\n",
    "                if i == pred_index: # only need once, since one sample corresponds to one sentence only\n",
    "                           # TODO Is that true? Why cannot caption all?\n",
    "                    # anchor length, 4, 5 are the indices for anchor length and center\n",
    "                    anc_len = prop_all[b, 4, pos_sam_ind].data.item()\n",
    "                    anc_cen = prop_all[b, 5, pos_sam_ind].data.item()\n",
    "\n",
    "                    # grount truth length, 2 and 3 are indices for ground truth length and center\n",
    "                    # see line 260 and 268 in anet_dataset.py\n",
    "                    # dont need to use index since now i is 0 and everything is matching\n",
    "                    # length is after taking log\n",
    "                    gt_len = np.exp(pos_sam[2].item()) * anc_len\n",
    "                    gt_cen = pos_sam[3].item() * anc_len + anc_cen\n",
    "                    gt_window_mask = torch.zeros(T, 1).type(dtype)\n",
    "\n",
    "                    gt_window_mask[\n",
    "                    max(0, math.floor(gt_cen - gt_len / 2.)):\n",
    "                    min(T, math.ceil(gt_cen + gt_len / 2.)), :] = 1.\n",
    "                    gt_window_mask = Variable(gt_window_mask,\n",
    "                                              requires_grad=False)\n",
    "\n",
    "                    batch_mask[b] = gt_window_mask\n",
    "                    # batch_mask[b] = anchor_window_mask\n",
    "\n",
    "                    crt_pred_cen = pred_cen[b, pos_sam_ind]\n",
    "                    crt_pred_len = pred_len[b, pos_sam_ind]\n",
    "                    pred_start_w = crt_pred_cen - crt_pred_len / 2.0\n",
    "                    pred_end_w = crt_pred_cen + crt_pred_len/2.0\n",
    "\n",
    "                    pred_bin_window_mask[b,\n",
    "                    math.floor(max(0, min(T-1, pred_start_w.data.item()))):\n",
    "                    math.ceil(max(1, min(T, pred_end_w.data.item())))] = 1.\n",
    "\n",
    "                    if self.learn_mask:\n",
    "                        anchor_window_mask[b,\n",
    "                        max(0, math.floor(anc_cen - anc_len / 2.)):\n",
    "                        min(T, math.ceil(anc_cen + anc_len / 2.))] = 1.\n",
    "\n",
    "                        pe_locs[b] = pred_start_w\n",
    "                        pe_locs[B + b] = pred_end_w\n",
    "                        pe_locs[B*2 + b] = Variable(torch.Tensor([max(0, math.floor(anc_cen - anc_len / 2.))]).type(dtype))\n",
    "                        pe_locs[B*3 + b] = Variable(torch.Tensor([min(T, math.ceil(anc_cen + anc_len / 2.))]).type(dtype))\n",
    "\n",
    "                        # gate_scores[b] = pred_score[b*sample_each+i, 0].detach()\n",
    "                        gate_scores[b] = pred_score[b*sample_each+i, 0]\n",
    "\n",
    "        #print(\"self.learn_mask: \", self.learn_mask)\n",
    "        if self.learn_mask:\n",
    "            pos_encs = positional_encodings(pe_locs, self.d_model//4)\n",
    "            in_pred_mask = torch.cat((pos_encs[:B], pos_encs[B:B*2],\n",
    "                                      pos_encs[B*2:B*3], pos_encs[B*3:B*4],\n",
    "                                      anchor_window_mask), 1)\n",
    "            pred_mask = self.mask_model(in_pred_mask).view(B, T, 1)\n",
    "\n",
    "            #print(\"gated_mask: \", gated_mask)\n",
    "            if gated_mask:\n",
    "                gate_scores = F.sigmoid(gate_scores)\n",
    "                window_mask = (gate_scores * pred_bin_window_mask.view(B, T, 1)\n",
    "                # window_mask = (gate_scores * batch_mask\n",
    "                                +  (1-gate_scores) * pred_mask)\n",
    "            else:\n",
    "                window_mask = pred_mask\n",
    "\n",
    "                \n",
    "            # Try to remove nan loss error\n",
    "            pred_mask.clamp (min=-10, max=10)\n",
    "            pred_bin_window_mask.clamp (min=-10, max=10)\n",
    "\n",
    "            mask_loss = F.binary_cross_entropy_with_logits(pred_mask + 1e-10, pred_bin_window_mask.view(B, T, 1) + 1e-10)\n",
    "            # mask_loss = F.binary_cross_entropy_with_logits(window_mask, batch_mask)\n",
    "        else:\n",
    "            window_mask = pred_bin_window_mask.view(B, T, 1)\n",
    "            # window_mask = batch_mask\n",
    "\n",
    "        #print(\"x: \", x.shape)\n",
    "        #print(\"sentence: \", sentence.shape, sentence)\n",
    "        #print(\"window mask: \", window_mask.shape)\n",
    "        #print(\"*\" * 50)\n",
    "        pred_sentence, gt_cent, in_encoding, h_from_decoder = self.cap_model(x, sentence, window_mask,\n",
    "                                               sample_prob=sample_prob)\n",
    "\n",
    "        scst_loss = None\n",
    "        #print(\"scst: \", scst)\n",
    "        if scst:\n",
    "            scst_loss = self.cap_model.scst(x, batch_mask, sentence)\n",
    "            \n",
    "        pred_sentence = pred_sentence\n",
    "            \n",
    "\n",
    "            \n",
    "        return (pred_score, gt_score,\n",
    "                pred_offsets, gt_offsets,\n",
    "                pred_sentence, gt_cent,\n",
    "                scst_loss, mask_loss, h_from_decoder)#, temporal_loss, attention_score)\n",
    "\n",
    "\n",
    "    def inference(self, x, actual_frame_length, sampling_sec,\n",
    "                  min_prop_num, max_prop_num,\n",
    "                  min_prop_num_before_nms, pos_thresh, stride_factor,\n",
    "                  gated_mask=False, using_recipe=False, recipe_vector = None, dual = False):\n",
    "        B, T, _ = x.size()\n",
    "        dtype = x.data.type()\n",
    "\n",
    "        #x_rgb, x_flow = torch.split(x, 2048, 2)\n",
    "        #x_rgb = self.rgb_emb(x_rgb.contiguous())\n",
    "        #x_flow = self.flow_emb(x_flow.contiguous())\n",
    "\n",
    "        #x = torch.cat((x_rgb, x_flow), 2)\n",
    "            \n",
    "        \n",
    "        if using_recipe and recipe_vector is not None:\n",
    "            #query_with_attention, temporal_loss, attention_score  = self.query_embed(recipe_vector, _final_tensor = vis_feat,\n",
    "            #                                                                         vids= vids, gold_segs = gold_segs)\n",
    "            x = torch.cat([x, recipe_vector], dim = -1)\n",
    "        x = self.dim_embedder(x)\n",
    "        vis_feat, all_emb = self.vis_emb(x, query=False, dual=False)\n",
    "        vis_feat = vis_feat.transpose(1,2).contiguous()\n",
    "\n",
    "        prop_lst = []\n",
    "        for i, kernel in enumerate(self.prop_out):\n",
    "\n",
    "            kernel_size = self.kernel_list[i]\n",
    "            if kernel_size <= actual_frame_length[0]: # no need to use larger kernel size in this case, batch size is only 1\n",
    "                pred_o = kernel(vis_feat)\n",
    "                anchor_c = Variable(torch.FloatTensor(np.arange(\n",
    "                    float(kernel_size)/2.0,\n",
    "                    float(T+1-kernel_size/2.0),\n",
    "                    math.ceil(kernel_size/stride_factor)\n",
    "                )).type(dtype))\n",
    "                if anchor_c.size(0) != pred_o.size(-1):\n",
    "                    raise Exception(\"size mismatch!\")\n",
    "\n",
    "                anchor_c = anchor_c.expand(B, 1, anchor_c.size(0))\n",
    "                anchor_l = Variable(torch.FloatTensor(anchor_c.size()).fill_(kernel_size).type(dtype).cuda())\n",
    "\n",
    "                pred_final = torch.cat((pred_o, anchor_l, anchor_c), 1)\n",
    "                prop_lst.append(pred_final)\n",
    "\n",
    "        prop_all = torch.cat(prop_lst, 2)\n",
    "\n",
    "        # assume 1st and 2nd are action prediction and overlap, respectively\n",
    "        prop_all[:,:2,:] = F.sigmoid(prop_all[:,:2,:])\n",
    "\n",
    "\n",
    "        pred_len = prop_all[:, 4, :] * torch.exp(prop_all[:, 2, :])\n",
    "        pred_cen = prop_all[:, 5, :] + prop_all[:, 4, :] * prop_all[:, 3, :]\n",
    "\n",
    "        nms_thresh_set = np.arange(0.9, 0.95, 0.05).tolist()\n",
    "        all_proposal_results = []\n",
    "\n",
    "        # store positional encodings, size of B x 4,\n",
    "        # the first B values are predicted starts,\n",
    "        # second B values are predicted ends,\n",
    "        # third B values are anchor starts,\n",
    "        # last B values are anchor ends\n",
    "        \n",
    "        for b in range(B):\n",
    "            crt_pred = prop_all.data[b]\n",
    "            crt_pred_cen = pred_cen.data[b]\n",
    "            crt_pred_len = pred_len.data[b]\n",
    "            pred_start_lst = [] #torch.zeros(B * 4).type(dtype)\n",
    "            pred_end_lst = []\n",
    "            anchor_start_lst = []\n",
    "            anchor_end_lst = []\n",
    "\n",
    "            pred_masks = []\n",
    "            batch_result = []\n",
    "            anchor_window_mask = [] #Variable(torch.zeros(B, T).type(dtype))\n",
    "            gate_scores = [] #Variable(torch.zeros(B, 1).type(dtype))\n",
    "            crt_nproposal = 0\n",
    "            nproposal = torch.sum(torch.gt(prop_all.data[b, 0, :], pos_thresh))\n",
    "            nproposal = min(max(nproposal, min_prop_num_before_nms),\n",
    "                            prop_all.size(-1))\n",
    "            pred_results = np.empty((nproposal, 3))\n",
    "            _, sel_idx = torch.topk(crt_pred[0], nproposal)\n",
    " \n",
    "            start_t = time.time()\n",
    "            for nms_thresh in nms_thresh_set:\n",
    "                for prop_idx in range(nproposal):\n",
    "                    #print(\"AAAAAAAAAAAA\")\n",
    "                    original_frame_len = actual_frame_length[b].item() + sampling_sec*2 # might be truncated at the end, hence + frame_to_second*2\n",
    "                    pred_start_w = crt_pred_cen[sel_idx[prop_idx]] - crt_pred_len[sel_idx[prop_idx]] / 2.0\n",
    "                    #print(\"pred_start_w: \", pred_start_w)\n",
    "                    pred_end_w = crt_pred_cen[sel_idx[prop_idx]] + crt_pred_len[sel_idx[prop_idx]] / 2.0\n",
    "                    pred_start = pred_start_w\n",
    "                    pred_end = pred_end_w\n",
    "                    if pred_start >= pred_end:\n",
    "                        continue\n",
    "                    if pred_end >= original_frame_len or pred_start < 0:\n",
    "                        continue\n",
    "\n",
    "                    hasoverlap = False\n",
    "                    if crt_nproposal > 0:\n",
    "                        if np.max(segment_iou(np.array([pred_start.cpu(), pred_end.cpu()]), pred_results[:crt_nproposal])) > nms_thresh:\n",
    "                            hasoverlap = True\n",
    "\n",
    "                    if not hasoverlap:\n",
    "                        pred_bin_window_mask = torch.zeros(1, T, 1).type(dtype)\n",
    "                        win_start = math.floor(max(min(pred_start, min(original_frame_len, T)-1), 0))\n",
    "                        win_end = math.ceil(max(min(pred_end, min(original_frame_len, T)), 1))\n",
    "                        # if win_start >= win_end:\n",
    "                        #     print('length: {}, mask window start: {} >= window end: {}, skipping'.format(\n",
    "                        #         original_frame_len, win_start, win_end,\n",
    "                        #     ))\n",
    "                        #     continue\n",
    "\n",
    "                        pred_bin_window_mask[:, win_start:win_end] = 1\n",
    "                        pred_masks.append(pred_bin_window_mask)\n",
    "\n",
    "                        if self.learn_mask:\n",
    "                            # 4, 5 are the indices for anchor length and center\n",
    "                            anc_len = crt_pred[4, sel_idx[prop_idx]]\n",
    "                            anc_cen = crt_pred[5, sel_idx[prop_idx]]\n",
    "                            # only use the pos sample to train, could potentially use more sample for training mask, but this is easier to do\n",
    "                            amask = torch.zeros(1,T).type(dtype)\n",
    "                            amask[0,\n",
    "                            max(0, math.floor(anc_cen - anc_len / 2.)):\n",
    "                            min(T, math.ceil(anc_cen + anc_len / 2.))] = 1.\n",
    "                            anchor_window_mask.append(amask)\n",
    "\n",
    "                            pred_start_lst.append(torch.Tensor([pred_start_w]).type(dtype).cuda())\n",
    "                            #print(\"pred_start_lst: \", pred_start_lst)\n",
    "                            pred_end_lst.append(torch.Tensor([pred_end_w]).type(dtype).cuda())\n",
    "                            anchor_start_lst.append(torch.Tensor([max(0,\n",
    "                                                                 math.floor(\n",
    "                                                                 anc_cen - anc_len / 2.))]).type(\n",
    "                                                                 dtype).cuda())\n",
    "                            anchor_end_lst.append(torch.Tensor([min(T,\n",
    "                                                               math.ceil(\n",
    "                                                               anc_cen + anc_len / 2.))]).type(\n",
    "                                                               dtype).cuda())\n",
    "\n",
    "                            gate_scores.append(torch.Tensor([crt_pred[0, sel_idx[prop_idx]]]).type(dtype).cuda())\n",
    "\n",
    "                        pred_results[crt_nproposal] = np.array([win_start,\n",
    "                                                                win_end,\n",
    "                                                                crt_pred[0, sel_idx[prop_idx]]])\n",
    "                        crt_nproposal += 1\n",
    "\n",
    "                    if crt_nproposal >= max_prop_num:\n",
    "                        break\n",
    "\n",
    "                if crt_nproposal >= min_prop_num:\n",
    "                    break\n",
    "\n",
    "            mid1_t = time.time()\n",
    "\n",
    "            if len(pred_masks) == 0: # append all-one window if no window is proposed\n",
    "                pred_masks.append(torch.ones(1, T, 1).type(dtype))\n",
    "                pred_results[0] = np.array([0, min(original_frame_len, T), pos_thresh])\n",
    "                crt_nproposal = 1\n",
    "\n",
    "            pred_masks = Variable(torch.cat(pred_masks, 0))\n",
    "            batch_x = x[b].unsqueeze(0).expand(pred_masks.size(0), x.size(1), x.size(2))\n",
    "\n",
    "            if self.learn_mask:\n",
    "                pe_pred_start = torch.cat(pred_start_lst, 0)\n",
    "                pe_pred_end = torch.cat(pred_end_lst, 0)\n",
    "                pe_anchor_start = torch.cat(anchor_start_lst, 0)\n",
    "                pe_anchor_end = torch.cat(anchor_end_lst, 0)\n",
    "\n",
    "                pe_locs = torch.cat((pe_pred_start, pe_pred_end, pe_anchor_start, pe_anchor_end), 0)\n",
    "                pos_encs = positional_encodings(pe_locs, self.d_model // 4)\n",
    "                npos = pos_encs.size(0)\n",
    "                anchor_window_mask = Variable(torch.cat(anchor_window_mask,dim =  0))\n",
    "                in_pred_mask = torch.cat((pos_encs[:npos//4], pos_encs[npos//4:npos//4*2],\n",
    "                                          pos_encs[npos//4 * 2:npos//4 * 3],\n",
    "                                          pos_encs[npos//4 * 3:npos//4 * 4],\n",
    "                                          anchor_window_mask), dim = 1)\n",
    "                pred_cont_masks  = self.mask_model(in_pred_mask).unsqueeze(2)\n",
    "\n",
    "                if gated_mask:\n",
    "                    gate_scores = Variable(torch.cat(gate_scores, 0).view(-1,1,1))\n",
    "                    window_mask = (gate_scores * pred_masks\n",
    "                                   + (1 - gate_scores) * pred_cont_masks)\n",
    "\n",
    "                else:\n",
    "                    window_mask = pred_cont_masks\n",
    "            else:\n",
    "                window_mask = pred_masks\n",
    "\n",
    "            mid2_t = time.time()\n",
    "\n",
    "            \n",
    "            pred_sentence = []\n",
    "            # use cap_batch as caption batch size\n",
    "            cap_batch = math.ceil(560*256/T)\n",
    "            for sent_i in range(math.ceil(window_mask.size(0)/cap_batch)):\n",
    "                batch_start = sent_i*cap_batch\n",
    "                batch_end = min((sent_i+1)*cap_batch, window_mask.size(0))\n",
    "                temp_len = batch_x[batch_start:batch_end].shape[0]\n",
    "                pred_sentence += self.cap_model.greedy(batch_x[batch_start:batch_end],\n",
    "                                                       window_mask[batch_start:batch_end],20)\n",
    "\n",
    "            pred_results = pred_results[:crt_nproposal]\n",
    "            assert len(pred_sentence) == crt_nproposal, (\n",
    "                \"number of predicted sentence and proposal does not match\"\n",
    "            )\n",
    "\n",
    "            for idx in range(len(pred_results)):\n",
    "                batch_result.append((pred_results[idx][0],\n",
    "                                     pred_results[idx][1],\n",
    "                                     pred_results[idx][2],\n",
    "                                     pred_sentence[idx]))\n",
    "            all_proposal_results.append(tuple(batch_result))\n",
    "\n",
    "            end_t = time.time()\n",
    "            #print('Processing time for tIoU: {:.2f}, mask: {:.2f}, caption: {:.2f}'.format(mid1_t-start_t, mid2_t-mid1_t, end_t-mid2_t))\n",
    "\n",
    "        return all_proposal_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(args):\n",
    "    print(\"Process Dataset\")\n",
    "    print(args.dataset_file)\n",
    "    # process text\n",
    "    text_proc, raw_data = get_vocab_and_sentences(args.dataset_file, args.max_sentence_len)\n",
    "    print(\"Create Dataset...\")\n",
    "    print(\"feature root: \", args.feature_root)\n",
    "    # Create the dataset and data loader instance\n",
    "    if 'deepct' in model_name:\n",
    "        recipt_pkl = \"/workspace/Dataset/lxmert_deepct_matching_recipe.pkl\"\n",
    "        recipt2bert_pkl = \"/workspace/Dataset/lxmert_deepct_matching_recipe2bert_onesent.pkl\"\n",
    "    elif 'match' in model_name:\n",
    "        recipt_pkl = \"/workspace/Dataset/lxmert_matching_recipe.pkl\"\n",
    "        recipt2bert_pkl = \"/workspace/Dataset/lxmert_matching_recipe2bert_onesent.pkl\"\n",
    "    else :\n",
    "        recipt_pkl = \"/workspace/Dataset/bm25_matching_recipe.pkl\"\n",
    "        recipt2bert_pkl = \"/workspace/Dataset/bm25_matching_recipe2bert_onesent.pkl\"\n",
    "    train_dataset = ANetDataset(args.feature_root,\n",
    "                                args.train_data_folder,\n",
    "                                args.slide_window_size,\n",
    "                                args.dur_file,\n",
    "                                args.kernel_list,\n",
    "                                text_proc, raw_data,\n",
    "                                args.pos_thresh, args.neg_thresh,\n",
    "                                args.stride_factor,\n",
    "                                args.dataset,\n",
    "                                save_samplelist=args.save_train_samplelist,\n",
    "                                load_samplelist=args.load_train_samplelist,\n",
    "                                sample_listpath=args.train_samplelist_path,\n",
    "                                recipt_pkl = recipt_pkl, recipt2bert_pkl = recipt2bert_pkl\n",
    "                                )\n",
    "\n",
    "    # dist parallel, optional\n",
    "    args.distributed = args.world_size > 1\n",
    "    if args.distributed and args.cuda:\n",
    "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                world_size=args.world_size)\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=args.batch_size,\n",
    "                              shuffle=(train_sampler is None), sampler=train_sampler,\n",
    "                              num_workers=args.num_workers,\n",
    "                              collate_fn=anet_collate_fn, drop_last = True)\n",
    "\n",
    "    valid_dataset = ANetDataset(args.feature_root,\n",
    "                                args.val_data_folder,\n",
    "                                args.slide_window_size,\n",
    "                                args.dur_file,\n",
    "                                args.kernel_list,\n",
    "                                text_proc, raw_data,\n",
    "                                args.pos_thresh, args.neg_thresh,\n",
    "                                args.stride_factor,\n",
    "                                args.dataset,\n",
    "                                save_samplelist=args.save_valid_samplelist,\n",
    "                                load_samplelist=args.load_valid_samplelist,\n",
    "                                sample_listpath=args.valid_samplelist_path,\n",
    "                                recipt_pkl = recipt_pkl, recipt2bert_pkl = recipt2bert_pkl\n",
    "                                )\n",
    "\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=args.valid_batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=args.num_workers,\n",
    "                              collate_fn=anet_collate_fn)\n",
    "\n",
    "    return train_loader, valid_loader, text_proc, train_sampler\n",
    "\n",
    "def get_model(text_proc, args):\n",
    "    sent_vocab = text_proc.vocab\n",
    "    model = ActionPropDenseCap(d_model=args.d_model,\n",
    "                               d_hidden=args.d_hidden,\n",
    "                               n_layers=args.n_layers,\n",
    "                               n_heads=args.n_heads,\n",
    "                               vocab=sent_vocab,\n",
    "                               in_emb_dropout=args.in_emb_dropout,\n",
    "                               attn_dropout=args.attn_dropout,\n",
    "                               vis_emb_dropout=args.vis_emb_dropout,\n",
    "                               cap_dropout=args.cap_dropout,\n",
    "                               nsamples=args.train_sample,\n",
    "                               kernel_list=args.kernel_list,\n",
    "                               stride_factor=args.stride_factor,\n",
    "                               learn_mask=args.mask_weight>0, window_length=args.slide_window_size)\n",
    "\n",
    "    # Initialize the networks and the criterion\n",
    "    if len(args.start_from) > 0:\n",
    "        print(\"Initializing weights from {}\".format(args.start_from))\n",
    "        model.load_state_dict(torch.load(args.start_from,\n",
    "                                              map_location=lambda storage, location: storage))\n",
    "\n",
    "    # Ship the model to GPU, maybe\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "        if args.distributed:\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "        # elif torch.cuda.device_count() > 1:\n",
    "        #     model = torch.nn.DataParallel(model).cuda()\n",
    "        # else:\n",
    "        #     model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Dataset\n",
      "/workspace/dense_cap/densecap/data/our/our_yc2_only_annotations_new_trainval.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_file:  /workspace/dense_cap/densecap/data/our/our_yc2_only_annotations_new_trainval.json\n",
      "# of words in the vocab: 841\n",
      "# of sentences in training: 7063, # of sentences in validation: 2472\n",
      "# of training videos: 910\n",
      "Create Dataset...\n",
      "feature root:  /workspace/dense_cap/our_dataset\n",
      "image_path:  /workspace/dense_cap/our_dataset\n",
      "split:  ['new_training']\n",
      "split paths:  ['/workspace/dense_cap/our_dataset/new_training']\n",
      "Load recipe dataset...\n",
      "image_path:  /workspace/dense_cap/our_dataset\n",
      "split:  ['new_validation']\n",
      "split paths:  ['/workspace/dense_cap/our_dataset/new_validation']\n",
      "Load recipe dataset...\n",
      "image_path:  /workspace/dense_cap/our_dataset\n",
      "split:  ['new_testing']\n",
      "split paths:  ['/workspace/dense_cap/our_dataset/new_testing']\n",
      "Load recipe dataset...\n"
     ]
    }
   ],
   "source": [
    "from scripts.train_our_copynet_adam_deepct_new_split import get_dataset\n",
    "\n",
    "train_loader, valid_loader, test_loader,text_proc, train_sampler = get_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6994, 2440, 1057)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset.sample_list), len(valid_loader.dataset.sample_list), len(test_loader.dataset.sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voca len:  841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "model = get_model(text_proc, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-02 12:38:55\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "writer = SummaryWriter('checkpoint/{}/{}'.format(model_name, timestamp))\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed adam to adamw\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            args.learning_rate, betas=(args.alpha, args.beta), eps=args.epsilon)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, factor=args.reduce_factor,\n",
    "                                               patience=args.patience_epoch,\n",
    "                                               verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7f85fd4a2848d389096fa49ae81230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: [581/582], training loss: 1.4910, class: 0.0471, reg: 0.1499, sentence: 1.2898, mask: 0.0041, data time: 319.6122s, total time: 320.0847s\n"
     ]
    }
   ],
   "source": [
    "best_loss = 1000\n",
    "vis, vis_window = None, None\n",
    "for epoch in tqdm(range(args.epoch)):\n",
    "    model.train() # training mode\n",
    "    train_loss = []\n",
    "    nbatches = len(train_loader)\n",
    "    t_iter_start = time.time()\n",
    "\n",
    "    sample_prob = min(args.sample_prob, int(epoch/5)*0.05)\n",
    "    scaler = GradScaler()\n",
    "    with autocast():\n",
    "        for train_iter, data in enumerate(train_loader):\n",
    "            (img_batch, tempo_seg_pos, tempo_seg_neg, sentence_batch, query, recipe_vector, neg_frame_flag, video_prefixes) = data\n",
    "            img_batch = Variable(img_batch)\n",
    "            tempo_seg_pos = Variable(tempo_seg_pos)\n",
    "            tempo_seg_neg = Variable(tempo_seg_neg)\n",
    "            sentence_batch = Variable(sentence_batch)\n",
    "\n",
    "            vids = [prefix.split('/')[-1] for prefix in video_prefixes]\n",
    "            bert_split = get_matched_recipe_feat(vids, duration_frame)\n",
    "            if args.cuda:\n",
    "                img_batch = img_batch.cuda()\n",
    "                tempo_seg_neg = tempo_seg_neg.cuda()\n",
    "                tempo_seg_pos = tempo_seg_pos.cuda()\n",
    "                sentence_batch = sentence_batch.cuda()\n",
    "                recipe_vector = recipe_vector.cuda()\n",
    "                neg_frame_flag =  neg_frame_flag.cuda()\n",
    "                bert_split = bert_split.cuda()\n",
    "            t_model_start = time.time()\n",
    "            batch_gold_seg = [list(gold_seg[vid].keys()) for vid in vids]\n",
    "            (pred_score, gt_score,\n",
    "            pred_offsets, gt_offsets,\n",
    "            pred_sentence, gt_sent,\n",
    "             scst_loss, mask_loss,_) = model(img_batch, tempo_seg_pos,\n",
    "                                           tempo_seg_neg, sentence_batch,\n",
    "                                           sample_prob, args.stride_factor,\n",
    "                                           scst=args.scst_weight > 0,\n",
    "                                           gated_mask=args.gated_mask, \n",
    "                                          using_recipe = args.using_recipe, recipe_vector = bert_split,\n",
    "                                            vids = vids, gold_segs = batch_gold_seg)\n",
    "            #gold_attention = build_gold_attention(batch_gold_seg)\n",
    "            #gold_attention = gold_attention.to(attention_score.device)\n",
    "            #attention_loss =  F.binary_cross_entropy_with_logits(attention_score + 1e-10, gold_attention+ 1e-10)\n",
    "            cls_loss = model.bce_loss(pred_score + 1e-10, gt_score + 1e-10) * args.cls_weight\n",
    "            reg_loss = model.reg_loss(pred_offsets + 1e-10, gt_offsets + 1e-10) * args.reg_weight\n",
    "\n",
    "            #print(\"pred_sentence\", pred_sentence, type(pred_sentence))\n",
    "            #print(\"gt_sent\", gt_sent, type(gt_sent))\n",
    "            ################################\n",
    "            ### Add dynamic sent_weight\n",
    "            ###############################\n",
    "            min_sent_weight = 0.5\n",
    "            min_sent_epoch = 15\n",
    "\n",
    "            #sent_loss = F.cross_entropy(pred_sentence + 1e-10 , gt_sent ) * args.sent_weight\n",
    "            tmp_sent_weight = args.sent_weight - (args.sent_weight - min_sent_weight) * min(1.0, epoch/min_sent_epoch)\n",
    "            sent_loss = F.cross_entropy(pred_sentence + 1e-10 , gt_sent ) * tmp_sent_weight\n",
    "            total_loss = cls_loss + reg_loss + sent_loss #+ attention_loss*attention_weight+ temporal_loss*temporal_weight\n",
    "\n",
    "            ################################\n",
    "            ### Add dynamic mask_weight\n",
    "            ###############################\n",
    "            min_mask_weight = 0.1\n",
    "            min_mask_epoch = 15\n",
    "\n",
    "            if scst_loss is not None:\n",
    "                scst_loss *= args.scst_weight\n",
    "                total_loss += scst_loss\n",
    "\n",
    "            if mask_loss is not None:\n",
    "                #mask_loss = args.mask_weight * mask_loss\n",
    "                mask_loss = (args.mask_weight - (args.mask_weight - min_mask_weight) * min(1.0, epoch / min_mask_epoch)) * mask_loss\n",
    "                total_loss += mask_loss\n",
    "            else:\n",
    "                mask_loss = cls_loss.new(1).fill_(0)\n",
    "            writer.add_scalar('train/cls_loss', cls_loss.item(),epoch*len(train_loader)+train_iter)\n",
    "            writer.add_scalar('train/sent_loss', sent_loss.item(),epoch*len(train_loader)+train_iter)\n",
    "            writer.add_scalar('train/reg_loss', reg_loss.item(),epoch*len(train_loader)+train_iter)\n",
    "            writer.add_scalar('train/total_loss', total_loss.item(),epoch*len(train_loader)+train_iter)\n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.append(total_loss.data.item())\n",
    "\n",
    "        t_model_end = time.time()\n",
    "        print('iter: [{}/{}], training loss: {:.4f}, '\n",
    "              'class: {:.4f}, '\n",
    "              'reg: {:.4f}, sentence: {:.4f}, '\n",
    "              'mask: {:.4f}, '\n",
    "              'data time: {:.4f}s, total time: {:.4f}s'.format(\n",
    "            train_iter, nbatches, total_loss.data.item(), cls_loss.data.item(),\n",
    "            reg_loss.data.item(), sent_loss.data.item(), mask_loss.data.item(),\n",
    "            t_model_start - t_iter_start,\n",
    "            t_model_end - t_iter_start\n",
    "        ), end='\\r')\n",
    "\n",
    "        t_iter_start = time.time()\n",
    "        valid_loss=[]\n",
    "        val_cls_loss=[]\n",
    "        val_reg_loss=[]\n",
    "        val_sent_loss=[]\n",
    "        val_mask_loss=[]\n",
    "        for iter_num, data in enumerate(valid_loader):\n",
    "            (img_batch, tempo_seg_pos, tempo_seg_neg, sentence_batch, query,  recipe_vector, neg_frame_flag, video_prefixes) = data\n",
    "            with torch.no_grad():\n",
    "                img_batch = Variable(img_batch)\n",
    "                tempo_seg_pos = Variable(tempo_seg_pos)\n",
    "                tempo_seg_neg = Variable(tempo_seg_neg)\n",
    "                sentence_batch = Variable(sentence_batch)\n",
    "                vids = [prefix.split('/')[-1] for prefix in video_prefixes]\n",
    "                batch_gold_seg = [list(gold_seg[vid].keys()) for vid in vids]\n",
    "                bert_split = get_matched_recipe_feat(vids, duration_frame)\n",
    "                if args.cuda:\n",
    "                    img_batch = img_batch.cuda()\n",
    "                    tempo_seg_neg = tempo_seg_neg.cuda()\n",
    "                    tempo_seg_pos = tempo_seg_pos.cuda()\n",
    "                    sentence_batch = sentence_batch.cuda()\n",
    "                    recipe_vector = recipe_vector.cuda()\n",
    "                    neg_frame_flag = neg_frame_flag.cuda()\n",
    "                    bert_split = bert_split.cuda()\n",
    "                (pred_score, gt_score,\n",
    "                 pred_offsets, gt_offsets,\n",
    "                 pred_sentence, gt_sent,\n",
    "                 _, mask_loss,_) = model(img_batch, tempo_seg_pos,\n",
    "                                        tempo_seg_neg, sentence_batch,\n",
    "                                        stride_factor=args.stride_factor,\n",
    "                                        gated_mask=args.gated_mask, \n",
    "                                       using_recipe = args.using_recipe, recipe_vector = bert_split,\n",
    "                                                                        vids = vids, gold_segs = batch_gold_seg)\n",
    "\n",
    "                cls_loss = model.bce_loss(pred_score + 1e-10, gt_score + 1e-10) * args.cls_weight\n",
    "                reg_loss = model.reg_loss(pred_offsets + 1e-10, gt_offsets + 1e-10) * args.reg_weight\n",
    "                sent_loss = F.cross_entropy(pred_sentence + 1e-10 , gt_sent) * args.sent_weight\n",
    "\n",
    "                total_loss = cls_loss + reg_loss + sent_loss #+ attention_loss*0.1+ temporal_loss*0.1\n",
    "\n",
    "                if mask_loss is not None:\n",
    "                    mask_loss = args.mask_weight * mask_loss\n",
    "                    total_loss += mask_loss\n",
    "                else:\n",
    "                    mask_loss = cls_loss.new(1).fill_(0)\n",
    "\n",
    "                valid_loss.append(total_loss.data.item())\n",
    "                val_cls_loss.append(cls_loss.data.item())\n",
    "                val_reg_loss.append(reg_loss.data.item())\n",
    "                val_sent_loss.append(sent_loss.data.item())\n",
    "                val_mask_loss.append(mask_loss.data.item())\n",
    "    writer.add_scalar('valid/cls_loss', np.mean(val_cls_loss),epoch)\n",
    "    writer.add_scalar('valid/sent_loss', np.mean(val_sent_loss),epoch)\n",
    "    writer.add_scalar('valid/reg_loss', np.mean(val_reg_loss),epoch)\n",
    "    writer.add_scalar('valid/total_loss', np.mean(valid_loss),epoch)\n",
    "    torch.save(model.state_dict(), os.path.join(model_name, 'epoch'+str(epoch)+'.pth'))\n",
    "    if np.mean(valid_loss)<best_loss:\n",
    "        torch.save(model.state_dict(), os.path.join(model_name, 'best_eval_loss.pth'))\n",
    "        best_loss = np.mean(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(model_name, 'best_eval_loss.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For evaluation captioning to tensorboard\n",
    "import shlex, subprocess\n",
    "def get_caption_scores(target):\n",
    "    scores = {}\n",
    "\n",
    "    flag = False;\n",
    "    meteor = None\n",
    "    Bleu_1 = None\n",
    "    Bleu_2 = None\n",
    "    Bleu_3 = None\n",
    "    Bleu_4 = None\n",
    "    CIDEr = None\n",
    "    ROUGE_L = None\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    for line in target.splitlines():    \n",
    "        if(\"Average across all tIoUs\" in line): flag = True; \n",
    "        elif(\"tIoU\" in line): flag = False\n",
    "\n",
    "        if(flag and \"METEOR\" in line):\n",
    "            metric = str(line.split(\":\")[0].split()[1])\n",
    "            score = float(line.split(\":\")[1])\n",
    "            \n",
    "            scores[metric] = score\n",
    "        elif(flag and \"Bleu_1\" in line):\n",
    "            metric = str(line.split(\":\")[0].split()[1])\n",
    "            score = float(line.split(\":\")[1])\n",
    "            \n",
    "            scores[metric] = score\n",
    "        elif(flag and \"Bleu_2\" in line):\n",
    "            metric = str(line.split(\":\")[0].split()[1])\n",
    "            score = float(line.split(\":\")[1])\n",
    "            \n",
    "            scores[metric] = score\n",
    "        elif(flag and \"Bleu_3\" in line):\n",
    "            metric = str(line.split(\":\")[0].split()[1])\n",
    "            score = float(line.split(\":\")[1])\n",
    "            \n",
    "            scores[metric] = score\n",
    "        elif(flag and \"Bleu_4\" in line):\n",
    "            metric = str(line.split(\":\")[0].split()[1])\n",
    "            score = float(line.split(\":\")[1])\n",
    "            \n",
    "            scores[metric] = score\n",
    "        elif(flag and \"CIDEr\" in line):\n",
    "            metric = str(line.split(\":\")[0].split()[1])\n",
    "            score = float(line.split(\":\")[1])\n",
    "            \n",
    "            scores[metric] = score\n",
    "        elif(flag and \"ROUGE_L\" in line):\n",
    "            metric = str(line.split(\":\")[0].split()[1])\n",
    "            score = float(line.split(\":\")[1])\n",
    "            \n",
    "            scores[metric] = score\n",
    "        elif(flag and \"Recall\" in line):\n",
    "            metric = str(line.split(\":\")[0].split()[1])\n",
    "            score = float(line.split(\":\")[1])\n",
    "            \n",
    "            scores[metric] = score\n",
    "        elif(flag and \"Precision\" in line):\n",
    "            metric = str(line.split(\":\")[0].split()[1])\n",
    "            score = float(line.split(\":\")[1])\n",
    "            \n",
    "            scores[metric] = score\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bac3278d6f49feb7121fcf02c32fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=59.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run eval\n"
     ]
    }
   ],
   "source": [
    "    with autocast():\n",
    "        densecap_result = {}\n",
    "        for data in tqdm(test_loader):\n",
    "                (img_batch, tempo_seg_pos, tempo_seg_neg, sentence_batch, query,  recipe_vector, neg_frame_flag, video_prefixes) = data\n",
    "                with torch.no_grad():\n",
    "                    img_batch = Variable(img_batch)\n",
    "                    tempo_seg_pos = Variable(tempo_seg_pos)\n",
    "                    tempo_seg_neg = Variable(tempo_seg_neg)\n",
    "                    sentence_batch = Variable(sentence_batch)\n",
    "                    vids = [prefix.split('/')[-1] for prefix in video_prefixes]\n",
    "                    #batch_gold_seg = [list(gold_seg[vid].keys()) for vid in vids]\n",
    "                    bert_split = get_matched_recipe_feat(vids, duration_frame)\n",
    "                    original_num_frame = torch.tensor([newof[prefix] for prefix in video_prefixes])\n",
    "                    if args.cuda:\n",
    "                        img_batch = img_batch.cuda()\n",
    "                        tempo_seg_neg = tempo_seg_neg.cuda()\n",
    "                        tempo_seg_pos = tempo_seg_pos.cuda()\n",
    "                        sentence_batch = sentence_batch.cuda()\n",
    "                        recipe_vector = recipe_vector.cuda()\n",
    "                        neg_frame_flag = neg_frame_flag.cuda()\n",
    "                        bert_split = bert_split.cuda()\n",
    "                    all_proposal_results = model.inference(img_batch,\n",
    "                                                           original_num_frame,\n",
    "                                                           sampling_sec,\n",
    "                                                           args.min_prop_num,\n",
    "                                                           args.max_prop_num,\n",
    "                                                           args.min_prop_before_nms,\n",
    "                                                           args.pos_thresh,\n",
    "                                                           args.stride_factor,\n",
    "                                                           gated_mask=args.gated_mask, \\\n",
    "                                                           using_recipe=args.using_recipe, recipe_vector = bert_split, dual=None)\n",
    "                    for b in range(len(video_prefixes)):\n",
    "                        vid = video_prefixes[b].split('/')[-1]\n",
    "                        densecap_result['v_'+vid] = []\n",
    "                        for pred_start, pred_end, pred_s, sent in all_proposal_results[b]:\n",
    "                            densecap_result['v_'+vid].append(\n",
    "                                {'sentence':sent,\n",
    "                                 'timestamp':[pred_start * sampling_sec,\n",
    "                                              pred_end * sampling_sec]})\n",
    "\n",
    "        ### Evaluation Captioning Start ....\n",
    "        ### From all collected captions\n",
    "        dense_cap_all = {'version':'VERSION 1.0', 'results':densecap_result,\n",
    "                         'external_data':{'used':'true',\n",
    "                          'details':'global_pool layer from BN-Inception pretrained from ActivityNet \\\n",
    "                                     and ImageNet (https://github.com/yjxiong/anet2016-cuhk)'}}\n",
    "        with open(os.path.join('../results/', 'densecap_'+args.val_data_folder[0]+'_'+args.id+ '.json'), 'w') as f:\n",
    "            json.dump(dense_cap_all, f)\n",
    "        print(\"Run eval\")\n",
    "        cmd = 'python3 ' + args.densecap_eval_file + \" -s \" + os.path.join('/workspace/dense_cap/results/', 'densecap_'+args.val_data_folder[0]+'_' + args.id + '.json') + \" -i \" + args.id + \" -v -r \" + args.densecap_references[0]\n",
    "        #!{cmd}   \n",
    "        split_cmd = shlex.split(cmd)\n",
    "\n",
    "        result = subprocess.run(split_cmd, stdout=subprocess.PIPE)\n",
    "        scores = get_caption_scores(result.stdout.decode())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('../results/', 'densecap_'+args.val_data_folder[0]+'_'+args.id+ '.json'), 'w') as f:\n",
    "            json.dump(dense_cap_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bleu_1': 23.6296,\n",
       " 'Bleu_2': 10.8107,\n",
       " 'Bleu_3': 4.1478,\n",
       " 'Bleu_4': 1.4483,\n",
       " 'METEOR': 8.6736,\n",
       " 'ROUGE_L': 23.1779,\n",
       " 'CIDEr': 25.5849,\n",
       " 'Recall': 71.552,\n",
       " 'Precision': 15.4092}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
