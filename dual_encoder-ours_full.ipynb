{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/workspace/evidence_retrieval/lxmert/src')\n",
    "from lxrt.modeling import *\n",
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import easydict\n",
    "from collections import OrderedDict\n",
    "import pickle as pkl\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'newfinal'\n",
    "exp_name = 'base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LXRTImageEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Obj-level image embedding layer\n",
    "        self.visn_fc = VisualFeatEncoder(config)\n",
    "        self.visn_cls = torch.nn.Parameter(torch.rand((1,768)))\n",
    "        # Number of layers\n",
    "        self.num_l_layers = VISUAL_CONFIG.l_layers\n",
    "        self.num_x_layers = VISUAL_CONFIG.x_layers\n",
    "        self.num_r_layers = VISUAL_CONFIG.r_layers\n",
    "        print(\"LXRT encoder with %d l_layers, %d x_layers, and %d r_layers.\" %\n",
    "              (self.num_l_layers, self.num_x_layers, self.num_r_layers))\n",
    "        self.visn_cls = torch.nn.Parameter(torch.rand((1,1,768)) , requires_grad=True)\n",
    "        # Layers\n",
    "        # Using self.layer instead of self.l_layer to support loading BERT weights.\n",
    "        self.r_layers = nn.ModuleList(\n",
    "            [BertLayer(config) for _ in range(self.num_r_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, lang_feats=None, lang_attention_mask=None,\n",
    "                visn_feats=None, visn_attention_mask=None):\n",
    "        # Run visual embedding layer\n",
    "        # Note: Word embedding layer was executed outside this module.\n",
    "        #       Keep this design to allow loading BERT weights.\n",
    "        visn_feats = self.visn_fc(visn_feats)\n",
    "        visn_cls = self.visn_cls.repeat((visn_feats.shape[0],1,1))\n",
    "        visn_feats = torch.cat((visn_cls, visn_feats),dim = -2 )\n",
    "        # Run cross-modality layers\n",
    "        for layer_module in self.r_layers:\n",
    "            visn_feats = layer_module(visn_feats, visn_attention_mask)\n",
    "\n",
    "        return visn_feats\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertPooler, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class LXRTImageModel(BertPreTrainedModel):\n",
    "    \"\"\"LXRT Model.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = LXRTImageEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids=None, token_type_ids=None, attention_mask=None,\n",
    "                visual_feats=None, visual_attention_mask=None):\n",
    "        if visual_attention_mask is not None:\n",
    "            extended_visual_attention_mask = visual_attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_visual_attention_mask = extended_visual_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "            extended_visual_attention_mask = (1.0 - extended_visual_attention_mask) * -10000.0\n",
    "        else:\n",
    "            extended_visual_attention_mask = None\n",
    "        # Run LXRT backbone\n",
    "        visn_feats = self.encoder(\n",
    "            None,\n",
    "            None,\n",
    "            visn_feats=visual_feats,\n",
    "            visn_attention_mask=extended_visual_attention_mask)\n",
    "        pooled_output = self.pooler(visn_feats)\n",
    "\n",
    "        return visn_feats, pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import base64\n",
    "class segmentDataset(Dataset):\n",
    "    \"\"\"Segment dataset.\"\"\"\n",
    "    def __init__(self, vidlistpkl, feat_base, cook2_IVD_dir =\"/workspace/evidence_retrieval/COOK2_IVD\", mode = 'train' ):\n",
    "        with open(os.path.join(vidlistpkl), \"rb\") as f:\n",
    "            vid_pkl = pkl.load(f)\n",
    "            \n",
    "        self.vid_list = vid_pkl\n",
    "        self.feat_base = feat_base\n",
    "        with open(os.path.join(cook2_IVD_dir, f\"pkl/{mode}.pkl\"), \"rb\") as f:\n",
    "            self.vid_pkl = pkl.load(f)\n",
    "    def __len__(self):\n",
    "        return len(self.vid_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "\n",
    "        vid = self.vid_list[idx]\n",
    "        with open(os.path.join(self.feat_base, vid+'.pkl'), 'rb') as fp:\n",
    "                  item = pkl.load(fp)\n",
    "        visn_feats, encode, temporal_label, label = item\n",
    "        query = self.vid_pkl[vid][\"query\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\"vid\": vid, \"visn_feats\":visn_feats, 'encode':encode, 'temporal_label':temporal_label,'label':label, 'query':query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_dataset = segmentDataset('newsplit_train_vid_list.pkl','feat_dump')\n",
    "valid_dataset = segmentDataset('newsplit_valid_vid_list.pkl','feat_dump', mode = 'vid')\n",
    "test_dataset = segmentDataset('newsplit_test_vid_list.pkl','feat_dump', mode = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"class Do_Calculus(nn.Module):\\n    def __init__(self, is_cuda = True, confounder_modality = 'lang'):\\n        super(Do_Calculus, self).__init__()\\n        with open(confounder_modality+'_confounder.pkl', 'rb') as fp:\\n            self.Z = pkl.load( fp)\\n        with open(confounder_modality+'_prior.pkl', 'rb') as fp:\\n            self.prior = pkl.load( fp)\\n        self.softmax = nn.Softmax(dim=-1)\\n        self.confounder_modaltiy = confounder_modality\\n        if confounder_modality == 'object':\\n            for key in self.Z.keys():\\n                if is_cuda:\\n                    self.Z[key] = torch.from_numpy(self.Z[key]).cuda().float()\\n                else:\\n                    self.Z[key] = torch.from_numpy(self.Z[key]).float()\\n            for key in self.prior.keys():\\n                if is_cuda:\\n                    self.prior[key] = torch.from_numpy(self.prior[key]).cuda().float()\\n                else:\\n                    self.prior[key] = torch.from_numpy(self.prior[key]).float()\\n            self.y_att_matrix = nn.Linear(768,768)\\n            self.z_att_matrix = nn.Linear(2048,768)\\n            self.e_z_x_transform = nn.Linear(2048,768)\\n        else:\\n            self.Z = torch.from_numpy(self.Z).cuda().float()\\n            self.prior = torch.from_numpy(self.prior).cuda().float()\\n            self.y_att_matrix = nn.Linear(768,768)\\n            self.z_att_matrix = nn.Linear(768,768)\\n            self.e_z_x_transform = nn.Linear(768,768)\\n    def forward(self, y, vid = None):\\n        if self.confounder_modaltiy == 'lang':\\n            y= self.y_att_matrix(y)\\n            z = self.z_att_matrix(self.Z)\\n            a = self.softmax(torch.matmul(y, z.transpose(1,0) ) )\\n            a = self.prior * a\\n            print(a.shape, print(self.Z[vid].shape))\\n            E_z_x = torch.matmul(a,self.Z[vid])\\n        else:\\n            y = self.y_att_matrix(y)\\n            z = self.z_att_matrix(self.Z[vid])\\n            a = self.softmax(torch.matmul(y, z.transpose(1,0)))\\n            a = self.prior[vid] * a        \\n            E_z_x = torch.matmul(a,self.Z[vid])\\n        E_z_x = self.e_z_x_transform(E_z_x)\\n        return E_z_x\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class prf_expansion(nn.Module):\n",
    "    def __init__(self, is_cuda = True, modality = 'lang'):\n",
    "        super(prf_expansion, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.confounder_modaltiy = modality\n",
    "        if confounder_modality == 'object':\n",
    "            self.Z = torch.from_numpy( np.load('dic_v.npy') ).cuda().float()\n",
    "            self.prior = torch.from_numpy(np.load('prior_v.npy')).cuda().float()\n",
    "            self.y_att_matrix = nn.Linear(768,768)\n",
    "            self.z_att_matrix = nn.Linear(2048,768) \n",
    "            self.e_z_x_transform = nn.Linear(2048,768)\n",
    "        else:\n",
    "            self.Z = torch.from_numpy( np.load('dic_t.npy') ).cuda().float()\n",
    "            self.prior = torch.from_numpy(np.load('prior_t.npy')).cuda().float()\n",
    "            self.y_att_matrix = nn.Linear(768,768)\n",
    "            self.z_att_matrix = nn.Linear(768,768)\n",
    "            self.e_z_x_transform = nn.Linear(768,768)\n",
    "    def forward(self, y, vid = None):\n",
    "            y = self.y_att_matrix(y)\n",
    "            z = self.z_att_matrix(self.Z)\n",
    "            a = self.softmax(torch.matmul(y, z.transpose(1,0)))\n",
    "            a = self.prior * a        \n",
    "            E_z_x = torch.matmul(a,self.Z)\n",
    "            E_z_x = self.e_z_x_transform(E_z_x)\n",
    "            return E_z_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class expansion_merge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Do_Evaluator, self).__init__()\n",
    "        \n",
    "        self.activation = nn.GELU()\n",
    "        self.m1 = nn.Linear(768*2,768, bias = True)\n",
    "        self.m2 = nn.Linear(768*2,768, bias = True)\n",
    "    def forward(self, x):\n",
    "        x = self.m1(x)\n",
    "        #x = self.activation(x)\n",
    "        #x = self.m2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yc2_recipes.json', 'r') as fp:\n",
    "    recipe_dict = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LXRT encoder with 9 l_layers, 5 x_layers, and 5 r_layers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig('bert_config.json')\n",
    "image_encoder = LXRTImageModel(config)\n",
    "state_dict_path = os.path.join('lxmert', 'snap', 'pretrained', 'model_LXRT.pth') \n",
    "state_dict = torch.load(state_dict_path)\n",
    "new_state_dict = OrderedDict()\n",
    "for key, value in state_dict.items():\n",
    "    splittedkey = key.split('.')\n",
    "    if 'bert' in splittedkey:\n",
    "        newkey  = '.'.join(splittedkey[splittedkey.index('bert')+1:])\n",
    "    else:\n",
    "        newkey  = '.'.join(splittedkey[splittedkey.index('module')+1:])\n",
    "    new_state_dict[newkey] = value\n",
    "image_encoder.load_state_dict(new_state_dict, strict=False)\n",
    "image_encoder.cuda().train()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers.models.bert.modeling_bert import BertOnlyMLMHead, BertConfig\n",
    "config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "lang_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "temporal_classifier = torch.nn.Linear(768*2,3).cuda().train()\n",
    "lang_encoder.cuda().train()\n",
    "expansion_obj = prf_expansion(modality = 'object')\n",
    "expansion_obj.cuda().train()\n",
    "expansion_word = prf_expansion(modality = 'lang')\n",
    "expansion_word.cuda().train()\n",
    "merge1 =  expansion_merge()\n",
    "merge1.cuda()\n",
    "merge2 =  expansion_merge()\n",
    "merge2.cuda()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxrt.optimization import BertAdam\n",
    "epoch = 100\n",
    "batch_per_epoch = len(segment_dataset)\n",
    "t_total = int(batch_per_epoch * epoch)\n",
    "warmup_ratio = 0.05\n",
    "warmup_iters = int(t_total * warmup_ratio)\n",
    "optim = BertAdam(list(image_encoder.parameters()) + list(lang_encoder.parameters())+list(temporal_classifier.parameters())+list(do_cal.parameters())+list(do_evaluator.parameters())+list(do_cal2.parameters())+list(do_evaluator2.parameters()),\n",
    "                       lr=1e-4, warmup=warmup_ratio, t_total=t_total)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_recipe = {}\n",
    "for foodname in recipe_dict.keys():\n",
    "    flatten_recipe[foodname] = []\n",
    "    for item in recipe_dict[foodname]:\n",
    "        flatten_recipe[foodname].extend(item['split_ins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-24 08:37:13\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "writer = SummaryWriter(f'{mode}/{exp_name}_{timestamp}')\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'base'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newfinal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/workspace/evidence_retrieval/lxmert/src/lxrt/optimization.py:142: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:87: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0.19477656702470966\n",
      "0 3 0.5197850611254626\n",
      "0 5 0.7503630852031805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:145: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  1%|          | 1/100 [03:30<5:46:47, 210.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0.18926839276841556\n",
      "0 3 0.5059511440939308\n",
      "0 5 0.7381105282081996\n",
      "1 1 0.20875102113822047\n",
      "1 3 0.5289107248720955\n",
      "1 5 0.7500553751300595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [07:00<5:43:17, 210.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 0.19679316042671882\n",
      "1 3 0.5175379612885255\n",
      "1 5 0.7484007330327531\n",
      "2 1 0.21040250497224755\n",
      "2 3 0.5465086995140811\n",
      "2 5 0.7697570276258016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [10:29<5:39:12, 209.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 0.2018633780941282\n",
      "2 3 0.5227114919581974\n",
      "2 5 0.7639056064392304\n",
      "3 1 0.26693942425924083\n",
      "3 3 0.5934126300003663\n",
      "3 5 0.8080714652428006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [13:59<5:35:48, 209.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1 0.25601512830092427\n",
      "3 3 0.5846159431278186\n",
      "3 5 0.8058208245440731\n",
      "4 1 0.2785332944588686\n",
      "4 3 0.6289419543104768\n",
      "4 5 0.8288953060671378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [17:28<5:32:02, 209.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1 0.26817561686608554\n",
      "4 3 0.6230110220617776\n",
      "4 5 0.8280048827835386\n",
      "5 1 0.2911223163915781\n",
      "5 3 0.6415884217634462\n",
      "5 5 0.8329179587949673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [20:58<5:28:47, 209.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1 0.2922538867091046\n",
      "5 3 0.6435747233217225\n",
      "5 5 0.8409589921493222\n",
      "6 1 0.2961511783833965\n",
      "6 3 0.645748812180138\n",
      "6 5 0.8343017384104536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [24:29<5:25:31, 210.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 1 0.3018925307396609\n",
      "6 3 0.639429899133046\n",
      "6 5 0.8363330877313226\n",
      "7 1 0.32415307719454856\n",
      "7 3 0.6707928320193501\n",
      "7 5 0.8511564237999217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [27:58<5:21:30, 209.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1 0.33116306415537\n",
      "7 3 0.6760722625231058\n",
      "7 5 0.8589055619672612\n",
      "8 1 0.33984923176678317\n",
      "8 3 0.6878111965175043\n",
      "8 5 0.8599057509483512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [31:26<5:17:24, 209.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 1 0.3421613725020853\n",
      "8 3 0.6816411013937719\n",
      "8 5 0.8612159539819239\n",
      "9 1 0.3423520310047439\n",
      "9 3 0.6812619594057262\n",
      "9 5 0.8572709126760832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [34:55<5:13:54, 209.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1 0.3433729014858584\n",
      "9 3 0.6799960745844703\n",
      "9 5 0.8586345300523562\n",
      "10 1 0.3528217454342772\n",
      "10 3 0.6991903214514869\n",
      "10 5 0.8640913082826653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [38:25<5:10:34, 209.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 0.3482542203092355\n",
      "10 3 0.6925376418128488\n",
      "10 5 0.8677541810106788\n",
      "11 1 0.35520756478720245\n",
      "11 3 0.7032953188597997\n",
      "11 5 0.8655819182889399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [41:54<5:07:09, 209.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 1 0.36393862988192116\n",
      "11 3 0.7092256705813361\n",
      "11 5 0.8667713208502705\n",
      "12 1 0.356821959500987\n",
      "12 3 0.7022532207822688\n",
      "12 5 0.8684353241657933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [45:24<5:03:44, 209.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 1 0.3522790962685266\n",
      "12 3 0.7058188868076927\n",
      "12 5 0.8748715541737242\n",
      "13 1 0.3634093517302202\n",
      "13 3 0.7006220550039698\n",
      "13 5 0.8679342345330286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [48:54<5:00:32, 209.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 1 0.3631712872698616\n",
      "13 3 0.706527966533062\n",
      "13 5 0.8683563763754814\n",
      "14 1 0.3771578908087649\n",
      "14 3 0.7111353225888936\n",
      "14 5 0.8698222136840624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [52:24<4:57:11, 209.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 1 0.36633519390613695\n",
      "14 3 0.717211416176506\n",
      "14 5 0.8725685671963618\n",
      "15 1 0.3836498157968173\n",
      "15 3 0.7162923687953121\n",
      "15 5 0.8740787981220896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [55:54<4:53:35, 209.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 1 0.38563710646196103\n",
      "15 3 0.7234648186806294\n",
      "15 5 0.8749571849810347\n",
      "16 1 0.3821812668014359\n",
      "16 3 0.7181458513946444\n",
      "16 5 0.8730526200921934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [59:24<4:50:13, 209.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 1 0.3815342770515268\n",
      "16 3 0.7257177662130136\n",
      "16 5 0.8716793289068009\n",
      "17 1 0.3767707148825731\n",
      "17 3 0.7068825362206359\n",
      "17 5 0.8696124261553976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [1:02:54<4:46:51, 209.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 1 0.3729724302180129\n",
      "17 3 0.7097179283825135\n",
      "17 5 0.8746096161008236\n",
      "18 1 0.3856262113833704\n",
      "18 3 0.7131795777786601\n",
      "18 5 0.8726483325799541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [1:06:22<4:42:44, 209.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 1 0.38029386015747657\n",
      "18 3 0.7200143667368986\n",
      "18 5 0.874131628463078\n",
      "19 1 0.3833101140495185\n",
      "19 3 0.7170034504340591\n",
      "19 5 0.8747538528107192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [1:09:50<4:38:42, 209.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 1 0.3850368324630197\n",
      "19 3 0.7270863510565776\n",
      "19 5 0.8745795628501808\n",
      "20 1 0.3859434540747815\n",
      "20 3 0.7218651426894724\n",
      "20 5 0.8772390171578488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [1:13:20<4:35:26, 209.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 1 0.38170475046367097\n",
      "20 3 0.7272167700437021\n",
      "20 5 0.8788008359193203\n",
      "21 1 0.38301719482300495\n",
      "21 3 0.7213708240240173\n",
      "21 5 0.8726928370419201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [1:16:49<4:31:52, 209.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 1 0.37992629367606046\n",
      "21 3 0.7305056743085517\n",
      "21 5 0.8782040203255759\n",
      "22 1 0.3836661275335266\n",
      "22 3 0.7192324372121427\n",
      "22 5 0.8741530584304014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [1:20:20<4:29:02, 209.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 1 0.3807502742441199\n",
      "22 3 0.7259576074334299\n",
      "22 5 0.8779649010536861\n",
      "23 1 0.39281508239982266\n",
      "23 3 0.7219740661973829\n",
      "23 5 0.8757734094212878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [1:23:50<4:25:47, 209.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 1 0.39642943092929456\n",
      "23 3 0.7226285046305423\n",
      "23 5 0.8752401518522226\n",
      "24 1 0.38526415927251656\n",
      "24 3 0.720452654327065\n",
      "24 5 0.8733486573161975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [1:27:21<4:22:40, 210.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 1 0.3828268345628145\n",
      "24 3 0.7228266409101481\n",
      "24 5 0.8692307916002335\n",
      "25 1 0.38521261898179165\n",
      "25 3 0.7223892034494648\n",
      "25 5 0.874149947264344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [1:30:47<4:17:50, 209.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 1 0.3843645208356483\n",
      "25 3 0.7293471732495509\n",
      "25 5 0.8748352449294785\n",
      "26 1 0.381945699136348\n",
      "26 3 0.7185700329337024\n",
      "26 5 0.8765655557217963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [1:34:14<4:13:26, 208.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 1 0.3695170442294796\n",
      "26 3 0.7225889934080472\n",
      "26 5 0.8785834283836932\n",
      "27 1 0.38084314407270176\n",
      "27 3 0.7177508121433073\n",
      "27 5 0.8713801462688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [1:37:41<4:09:33, 207.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 1 0.3755618255354009\n",
      "27 3 0.7164948342892488\n",
      "27 5 0.867994281540041\n",
      "28 1 0.3857877869576549\n",
      "28 3 0.7231187594307289\n",
      "28 5 0.8775029122070592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [1:41:07<4:05:20, 207.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 1 0.38746484955485483\n",
      "28 3 0.7236069218769198\n",
      "28 5 0.8767372803669744\n",
      "29 1 0.3836057660531\n",
      "29 3 0.7169043307646807\n",
      "29 5 0.8728946455804806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [1:44:34<4:01:50, 207.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 1 0.38367041967368837\n",
      "29 3 0.7188926877969849\n",
      "29 5 0.8710106921414864\n",
      "30 1 0.3822098620314254\n",
      "30 3 0.7161348760227947\n",
      "30 5 0.8743561760392154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [1:47:56<3:56:36, 205.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 1 0.3831015128054702\n",
      "30 3 0.7140037035549454\n",
      "30 5 0.8710464462739479\n",
      "31 1 0.3857063988483347\n",
      "31 3 0.7154560207173546\n",
      "31 5 0.8714510867616249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [1:51:22<3:53:01, 205.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 1 0.37676994914099965\n",
      "31 3 0.7110742065808\n",
      "31 5 0.869031345668396\n",
      "32 1 0.3779654573569768\n",
      "32 3 0.7133010218860162\n",
      "32 5 0.8710677920660717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [1:54:47<3:49:36, 205.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 1 0.3807602899400786\n",
      "32 3 0.7075738862189364\n",
      "32 5 0.8676465996067877\n",
      "33 1 0.38317745215174054\n",
      "33 3 0.7133054529726621\n",
      "33 5 0.8708441137400206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [1:58:14<3:46:24, 205.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 1 0.3841968899936905\n",
      "33 3 0.7116671827344211\n",
      "33 5 0.8649857839758419\n",
      "34 1 0.3779446842640757\n",
      "34 3 0.7146506059103374\n",
      "34 5 0.8681517512279183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [2:01:38<3:42:31, 205.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 1 0.38342971528824765\n",
      "34 3 0.7139709077121212\n",
      "34 5 0.8651861949529288\n",
      "35 1 0.37965725626514174\n",
      "35 3 0.7179952415933014\n",
      "35 5 0.8702217890900094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [2:05:01<3:38:23, 204.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 1 0.3729909245195554\n",
      "35 3 0.7173512576235255\n",
      "35 5 0.8699546548206784\n",
      "36 1 0.38653221810994914\n",
      "36 3 0.7137812141148324\n",
      "36 5 0.8716930754961754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [2:08:29<3:35:50, 205.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 1 0.38962162481337864\n",
      "36 3 0.7099786645993665\n",
      "36 5 0.8705271243863997\n",
      "37 1 0.3800466081036877\n",
      "37 3 0.7167622357937767\n",
      "37 5 0.8722976214165831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [2:11:55<3:32:39, 205.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 1 0.38360877074599425\n",
      "37 3 0.7192774354168584\n",
      "37 5 0.8686647630343299\n",
      "38 1 0.378526188457327\n",
      "38 3 0.7097151042124822\n",
      "38 5 0.8710682404788503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [2:15:21<3:29:14, 205.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 1 0.3810581339150242\n",
      "38 3 0.704467043797446\n",
      "38 5 0.8662019351750878\n",
      "39 1 0.3791505142445169\n",
      "39 3 0.713274425980578\n",
      "39 5 0.8717523608923384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [2:18:47<3:25:53, 205.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 1 0.381258322003126\n",
      "39 3 0.7114110680389123\n",
      "39 5 0.8698238047715918\n",
      "40 1 0.38315004883550496\n",
      "40 3 0.70957857706991\n",
      "40 5 0.8739113285183293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [2:22:15<3:23:04, 206.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 1 0.38103207514447557\n",
      "40 3 0.7063957784468669\n",
      "40 5 0.8710190988918098\n",
      "41 1 0.37996269586392734\n",
      "41 3 0.7139687589500656\n",
      "41 5 0.8718292308176016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [2:25:43<3:20:12, 207.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 1 0.37912549940947526\n",
      "41 3 0.7127001821152786\n",
      "41 5 0.869518684846762\n",
      "42 1 0.37254825478795855\n",
      "42 3 0.7078974665507107\n",
      "42 5 0.8698979047911715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [2:29:09<3:16:25, 206.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 1 0.3748598596977842\n",
      "42 3 0.7079154927135275\n",
      "42 5 0.8689391039098595\n",
      "43 1 0.3818575205463893\n",
      "43 3 0.7149025769665149\n",
      "43 5 0.8741523522064076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [2:32:36<3:12:54, 206.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 1 0.3806429874920316\n",
      "43 3 0.7139877519217754\n",
      "43 5 0.872364741202292\n",
      "44 1 0.38217711234834933\n",
      "44 3 0.7110328766168239\n",
      "44 5 0.8731742964722992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [2:36:01<3:09:03, 206.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 1 0.3790607114805931\n",
      "44 3 0.7101960776602768\n",
      "44 5 0.8736723232917789\n",
      "45 1 0.3764237887762211\n",
      "45 3 0.7103586416666275\n",
      "45 5 0.8742776827891927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [2:39:26<3:05:15, 205.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 1 0.3760200500247392\n",
      "45 3 0.7109603229942136\n",
      "45 5 0.8722979320829839\n",
      "46 1 0.3742527057891732\n",
      "46 3 0.709223917834848\n",
      "46 5 0.8675517845765602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [2:42:52<3:01:53, 205.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46 1 0.3779712382693172\n",
      "46 3 0.7061787317785898\n",
      "46 5 0.8668615938270727\n",
      "47 1 0.373715153255752\n",
      "47 3 0.7059531632064326\n",
      "47 5 0.8688262038395411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [2:46:18<2:58:23, 205.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 1 0.37249747617528384\n",
      "47 3 0.701143575331323\n",
      "47 5 0.8687216752474763\n",
      "48 1 0.3773737831493294\n",
      "48 3 0.7067572932297334\n",
      "48 5 0.8682257607985928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [2:49:43<2:54:53, 205.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 1 0.37653976376286075\n",
      "48 3 0.7067559280054448\n",
      "48 5 0.8632316239183955\n",
      "49 1 0.3752092285190204\n",
      "49 3 0.7053793537129163\n",
      "49 5 0.86724834829635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [2:53:10<2:51:34, 205.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 1 0.37663342082330786\n",
      "49 3 0.707012971251291\n",
      "49 5 0.8648116716339261\n",
      "50 1 0.3809976856062606\n",
      "50 3 0.7102192600905745\n",
      "50 5 0.8719248003945109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [2:56:35<2:48:05, 205.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 1 0.38665342546507536\n",
      "50 3 0.7127383954739884\n",
      "50 5 0.8707332777917545\n",
      "51 1 0.3773624082691862\n",
      "51 3 0.7105463474439716\n",
      "51 5 0.8698462505888227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [2:59:58<2:43:52, 204.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 1 0.3729910286114342\n",
      "51 3 0.7069412810884488\n",
      "51 5 0.865071602241693\n",
      "52 1 0.3753039208662643\n",
      "52 3 0.7081302934784505\n",
      "52 5 0.8681185138750337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [3:03:24<2:40:43, 205.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 1 0.3789575993287441\n",
      "52 3 0.70152190597326\n",
      "52 5 0.8629447221206882\n",
      "53 1 0.3676792379245922\n",
      "53 3 0.703442955932083\n",
      "53 5 0.8684308531641247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [3:06:48<2:37:03, 204.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 1 0.37011533008279995\n",
      "53 3 0.7029823318913986\n",
      "53 5 0.8668548214001714\n",
      "54 1 0.3692651637346985\n",
      "54 3 0.7050394096176521\n",
      "54 5 0.8666502678051623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [3:10:15<2:34:05, 205.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 1 0.36428348146259737\n",
      "54 3 0.7002228719346544\n",
      "54 5 0.8616480948652985\n",
      "55 1 0.3689712713510545\n",
      "55 3 0.7010102299170461\n",
      "55 5 0.8664762777915774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [3:13:41<2:30:51, 205.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 1 0.3772447813325952\n",
      "55 3 0.7024914530027512\n",
      "55 5 0.8632570622717489\n",
      "56 1 0.3735037793539117\n",
      "56 3 0.7093836752613406\n",
      "56 5 0.8695202673142802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [3:17:07<2:27:35, 205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 1 0.37705773821866473\n",
      "56 3 0.7098180219685021\n",
      "56 5 0.8669172135317914\n",
      "57 1 0.3681102222427377\n",
      "57 3 0.7074765466061018\n",
      "57 5 0.8663109728560164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [3:20:33<2:24:09, 205.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 1 0.3660917362520982\n",
      "57 3 0.7096416490716229\n",
      "57 5 0.8639035940724854\n",
      "58 1 0.3793028915140154\n",
      "58 3 0.7085297321694066\n",
      "58 5 0.8698986042309493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [3:23:57<2:20:15, 205.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 1 0.3820391848345664\n",
      "58 3 0.7061899792786739\n",
      "58 5 0.8635434583280149\n",
      "59 1 0.373988329955571\n",
      "59 3 0.7046222534040003\n",
      "59 5 0.8692747959744046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [3:27:23<2:16:56, 205.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 1 0.37780542004393264\n",
      "59 3 0.7011760785690387\n",
      "59 5 0.861968357859748\n",
      "60 1 0.36981083152711064\n",
      "60 3 0.7023295008656388\n",
      "60 5 0.8685180432809667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [3:30:49<2:13:33, 205.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 1 0.3723090105334006\n",
      "60 3 0.6970740421225579\n",
      "60 5 0.865022661219864\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "min_loss = 0\n",
    "sampling_num = 3\n",
    "temporal_weight = 0.1\n",
    "print(mode)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    random_idx = list(range(len(segment_dataset)))\n",
    "    random.shuffle(random_idx)\n",
    "    for batch_idx in range(len(segment_dataset)):\n",
    "        batch = segment_dataset.__getitem__(random_idx[batch_idx])\n",
    "        vid, visn_feats, encode, temporal_label, label = batch['vid'], batch[\"visn_feats\"],batch['encode'], batch['temporal_label'],batch['label']\n",
    "        frame_feats, box_feats = visn_feats\n",
    "        foodname = batch['query']\n",
    "        optim.zero_grad()\n",
    "        image_encoder.train()\n",
    "        lang_encoder.train()\n",
    "        expansion_obj.train()\n",
    "        expansion_word.train()\n",
    "        merge1.train()\n",
    "        merge2.train()\n",
    "        temporal_classifier.train()\n",
    "        with autocast():\n",
    "            temporal_label = torch.tensor(temporal_label).long().cuda()\n",
    "            label = torch.tensor(label).long().cuda()\n",
    "            visn_feats = torch.tensor(frame_feats).float().cuda(), torch.tensor(box_feats).float().cuda()\n",
    "            seq_output, pooled_output = image_encoder(visual_feats = visn_feats)\n",
    "            encode = encode.cuda()\n",
    "            output = lang_encoder(encode)\n",
    "            sequence_output, lang_pooled_output = output[0], output[1]\n",
    "            expanded_word = expansion_word(pooled_output, vid = vid)\n",
    "            concat1 = torch.cat(( pooled_output, expanded_word), dim = -1)\n",
    "            concat1 = merge1(concat1)\n",
    "            expanded_obj = expansion_obj(lang_pooled_output, vid = vid)\n",
    "            concat2 = torch.cat((lang_pooled_output, do_output2), dim = -1)\n",
    "            concat2 = merge2(concat2)\n",
    "            dotoutput = torch.matmul(concat1, concat2.transpose(1,0))\n",
    "            loss = loss_fn(dotoutput, label)\n",
    "            writer.add_scalar('Loss/train',loss.detach().cpu(), epoch*len(segment_dataset)+batch_idx)\n",
    "            temporal_sample = []\n",
    "            for visn_i in range(len(pooled_output)):\n",
    "                    for lang_i in range(len(lang_pooled_output)):\n",
    "                        temporal_sample.append(torch.cat((pooled_output[visn_i],\n",
    "                                                          lang_pooled_output[lang_i]), -1).unsqueeze(0))\n",
    "            temporal_sample = torch.cat(temporal_sample, 0)\n",
    "            temporal_output = temporal_classifier(temporal_sample)\n",
    "            temporal_loss = loss_fn(temporal_output, temporal_label)\n",
    "            loss = loss+temporal_weight*temporal_loss\n",
    "            writer.add_scalar('Loss/train_total',loss.detach().cpu(), epoch*len(segment_dataset)+batch_idx)\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(list(image_encoder.parameters()) + list(lang_encoder.parameters())+list(temporal_classifier.parameters())+list(do_cal.parameters())+list(do_evaluator.parameters()),1 )\n",
    "        scaler.step(optim)\n",
    "        scaler.update()\n",
    "    torch.save(lang_encoder.state_dict(), os.path.join(f'{mode}','lang_encoder_epoch{}.pth'.format(epoch)))\n",
    "    torch.save(image_encoder.state_dict(),  os.path.join(f'{mode}','image_encoder_epoch{}.pth'.format(epoch)))\n",
    "    torch.save(expansion_obj.state_dict(), os.path.join(f'{mode}','do_cal_epoch{}.pth'.format(epoch)))\n",
    "    torch.save(expansion_word.state_dict(), os.path.join(f'{mode}','do_eval_{}.pth'.format(epoch)))\n",
    "    torch.save(merge1.state_dict(),os.path.join(f'{mode}','do_cal2_epoch{}.pth'.format(epoch)))\n",
    "    torch.save(merge2.state_dict(), os.path.join(f'{mode}','do_eval2_{}.pth'.format(epoch)))\n",
    "    with torch.no_grad():\n",
    "            ks = [1,3,5]\n",
    "            recalls = {k:[] for k in ks}\n",
    "            image_encoder.eval()\n",
    "            lang_encoder.eval()\n",
    "            expansion_obj.eval()\n",
    "            expansion_word.eval()\n",
    "            merge1.eval()\n",
    "            merge2.eval()\n",
    "            temporal_classifier.eval()\n",
    "            with autocast():\n",
    "                losses = 0\n",
    "                for batch_idx in range(len(valid_dataset)):\n",
    "                    batch = valid_dataset.__getitem__(batch_idx)\n",
    "                    vid, visn_feats, encode, temporal_label, label = batch['vid'], batch[\"visn_feats\"],batch['encode'], batch['temporal_label'],batch['label']\n",
    "                    frame_feats, box_feats = visn_feats\n",
    "                    foodname = batch['query']\n",
    "                    temporal_label = torch.tensor(temporal_label).long().cuda()\n",
    "                    label = torch.tensor(label).long().cuda()\n",
    "                    visn_feats = torch.tensor(frame_feats).float().cuda(), torch.tensor(box_feats).float().cuda()\n",
    "                    seq_output, pooled_output = image_encoder(visual_feats = visn_feats)\n",
    "                    encode = encode.cuda()\n",
    "                    output = lang_encoder(encode)\n",
    "                    sequence_output, lang_pooled_output = output[0], output[1]\n",
    "                    expanded_word = expansion_word(pooled_output, vid = vid)\n",
    "                    concat1 = torch.cat(( pooled_output, expanded_word), dim = -1)\n",
    "                    concat1 = merge1(concat1)\n",
    "                    expanded_obj = expansion_obj(lang_pooled_output, vid = vid)\n",
    "                    concat2 = torch.cat((lang_pooled_output, do_output2), dim = -1)\n",
    "                    concat2 = merge2(concat2)\n",
    "                    dotoutput = torch.matmul(concat1, concat2.transpose(1,0))\n",
    "                    label = list(label.detach().cpu().numpy())\n",
    "                    for k in ks:\n",
    "                        pred = np.argsort(-1*dotoutput.detach().cpu().numpy(), axis = -1)[:,:k].squeeze()\n",
    "                        recallatk = 0\n",
    "                        examples = 0\n",
    "                        for i, gt in enumerate(label):\n",
    "                            if k > 1:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt in pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                            else:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt == pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                        recallatk = recallatk/examples\n",
    "                        recalls[k].append(recallatk)\n",
    "                for k in ks:\n",
    "                    print(epoch, k, sum(recalls[k])/len(recalls[k]))\n",
    "                    writer.add_scalar(f'Recall/{k}',sum(recalls[k])/len(recalls[k]), epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load trained model\n",
    "epoch = 23\n",
    "lang_encoder.load_state_dict(torch.load(os.path.join(f'{mode}','lang_encoder_epoch{}.pth'.format(epoch))))\n",
    "image_encoder.load_state_dict(torch.load(  os.path.join(f'{mode}','image_encoder_epoch{}.pth'.format(epoch))))\n",
    "expansion_obj.load_state_dict(torch.load(os.path.join(f'{mode}','expansion_obj_epoch{}.pth'.format(epoch))))\n",
    "expansion_word.load_state_dict(torch.load( os.path.join(f'{mode}','expansion_word_{}.pth'.format(epoch))))\n",
    "merge1.load_state_dict(torch.load(os.path.join(f'{mode}','merge1_epoch{}.pth'.format(epoch))))\n",
    "merge2.load_state_dict(torch.load( os.path.join(f'{mode}','merge2_{}.pth'.format(epoch))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  app.launch_new_instance()\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 1 0.39642943092929456\n",
      "23 3 0.7226285046305423\n",
      "23 5 0.8752401518522226\n"
     ]
    }
   ],
   "source": [
    "    #eval recall on test set\n",
    "    with torch.no_grad():\n",
    "            ks = [1,3,5]\n",
    "            recalls = {k:[] for k in ks}\n",
    "            image_encoder.eval()\n",
    "            lang_encoder.eval()\n",
    "            expansion_obj.eval()\n",
    "            expansion_word.eval()\n",
    "            merge1.eval()\n",
    "            merge2.eval()\n",
    "            with autocast():\n",
    "                losses = 0\n",
    "                for batch_idx in range(len(test_dataset)):\n",
    "                    batch = valid_dataset.__getitem__(batch_idx)\n",
    "                    vid, visn_feats, encode, temporal_label, label = batch['vid'], batch[\"visn_feats\"],batch['encode'], batch['temporal_label'],batch['label']\n",
    "                    frame_feats, box_feats = visn_feats\n",
    "                    foodname = batch['query']\n",
    "                    temporal_label = torch.tensor(temporal_label).long().cuda()\n",
    "                    label = torch.tensor(label).long().cuda()\n",
    "                    visn_feats = torch.tensor(frame_feats).float().cuda(), torch.tensor(box_feats).float().cuda()\n",
    "                    seq_output, pooled_output = image_encoder(visual_feats = visn_feats)\n",
    "                    encode = encode.cuda()\n",
    "                    output = lang_encoder(encode)\n",
    "                    sequence_output, lang_pooled_output = output[0], output[1]\n",
    "                    expanded_word = expansion_word(pooled_output, vid = vid)\n",
    "                    concat1 = torch.cat(( pooled_output, expanded_word), dim = -1)\n",
    "                    concat1 = merge1(concat1)\n",
    "                    expanded_obj = expansion_obj(lang_pooled_output, vid = vid)\n",
    "                    concat2 = torch.cat((lang_pooled_output, do_output2), dim = -1)\n",
    "                    concat2 = merge2(concat2)\n",
    "                    dotoutput = torch.matmul(concat1, concat2.transpose(1,0))\n",
    "                    label = list(label.detach().cpu().numpy())\n",
    "                    for k in ks:\n",
    "                        pred = np.argsort(-1*dotoutput.detach().cpu().numpy(), axis = -1)[:,:k].squeeze()\n",
    "                        recallatk = 0\n",
    "                        examples = 0\n",
    "                        for i, gt in enumerate(label):\n",
    "                            if k > 1:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt in pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                            else:\n",
    "                                if gt == -1:\n",
    "                                    continue\n",
    "                                if gt == pred[i]:\n",
    "                                    recallatk +=1\n",
    "                                examples += 1\n",
    "                        recallatk = recallatk/examples\n",
    "                        recalls[k].append(recallatk)\n",
    "                for k in ks:\n",
    "                    print(epoch, k, sum(recalls[k])/len(recalls[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import base64\n",
    "cook2_IVD_dir = \"/workspace/evidence_retrieval/COOK2_IVD\"\n",
    "\n",
    "\n",
    "with open(os.path.join(cook2_IVD_dir, \"pkl/{}.pkl\".format(\"train\")), \"rb\") as f:\n",
    "    vid_pkl = pkl.load(f)\n",
    "    \n",
    "csv.field_size_limit(sys.maxsize)\n",
    "FIELDNAMES = [\"img_id\", \"img_h\", \"img_w\", \"objects_id\", \"objects_conf\",\n",
    "              \"attrs_id\", \"attrs_conf\", \"num_boxes\", \"boxes\", \"features\"]\n",
    "\n",
    "\n",
    "def load_obj_tsv(fname, topk=None):\n",
    "    \"\"\"Load object features from tsv file.\n",
    "    :param fname: The path to the tsv file.\n",
    "    :param topk: Only load features for top K images (lines) in the tsv file.\n",
    "        Will load all the features if topk is either -1 or None.\n",
    "    :return: A list of image object features where each feature is a dict.\n",
    "        See FILENAMES above for the keys in the feature dict.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    start_time = time.time()\n",
    "    #print(\"Start to load Faster-RCNN detected objects from %s\" % fname)\n",
    "    with open(fname) as f:\n",
    "        reader = csv.DictReader(f, FIELDNAMES, delimiter=\"\\t\")\n",
    "        for i, item in enumerate(reader):\n",
    "\n",
    "            for key in ['img_h', 'img_w', 'num_boxes']:\n",
    "                item[key] = int(item[key])\n",
    "            \n",
    "            boxes = item['num_boxes']\n",
    "            decode_config = [\n",
    "                ('objects_id', (boxes, ), np.int64),\n",
    "                ('objects_conf', (boxes, ), np.float32),\n",
    "                ('attrs_id', (boxes, ), np.int64),\n",
    "                ('attrs_conf', (boxes, ), np.float32),\n",
    "                ('boxes', (boxes, 4), np.float32),\n",
    "                ('features', (boxes, -1), np.float32),\n",
    "            ]\n",
    "            for key, shape, dtype in decode_config:\n",
    "                item[key] = np.frombuffer(base64.b64decode(item[key]), dtype=dtype)\n",
    "                item[key] = item[key].reshape(shape)\n",
    "                item[key].setflags(write=False)\n",
    "\n",
    "            data.append(item)\n",
    "            if topk is not None and len(data) == topk:\n",
    "                break\n",
    "    elapsed_time = time.time() - start_time\n",
    "    #print(\"Loaded %d images in file %s in %d seconds.\" % (len(data), fname, elapsed_time))\n",
    "    return data\n",
    "\n",
    "class segmentDataset(Dataset):\n",
    "    \"\"\"Segment dataset.\"\"\"\n",
    "    def __init__(self, cook2_IVD_dir, mode = \"train\"):\n",
    "        with open(os.path.join(cook2_IVD_dir, \"pkl/{}.pkl\".format(mode)), \"rb\") as f:\n",
    "            vid_pkl = pkl.load(f)\n",
    "        \n",
    "        self.vid_list = list(vid_pkl.keys())\n",
    "        self.vid_pkl = vid_pkl\n",
    "\n",
    "        self.max_frame_len = 552 ### IF you change this, segment_collate also needs to change !!!!!!\n",
    "    def __len__(self):\n",
    "        return len(self.vid_pkl)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        vid = self.vid_list[idx]\n",
    "        max_frame_len = self.max_frame_len\n",
    "        \n",
    "        index_transcript = self.vid_pkl[vid][\"index_trainscirpt\"]\n",
    "        segments_ = self.vid_pkl[vid][\"segments\"]\n",
    "        \n",
    "        trans_idx = torch.tensor([idx for idx, trans in index_transcript])\n",
    "        trans = [trans for idx, trans in index_transcript]\n",
    "        \n",
    "        segs_idx = torch.tensor([(idx_start, idx_end) for idx_start, idx_end, segs in segments_])\n",
    "        segs = [segs for idx_start, idx_end, segs in segments_]\n",
    "        \n",
    "        begin_idxs = list( int(b.cpu().tolist()) for b, e in segs_idx)\n",
    "        end_idxs = list( int(e.cpu().tolist()) for b, e in segs_idx)\n",
    "\n",
    "        \n",
    "        query = self.vid_pkl[vid][\"query\"]\n",
    "        frame_info = load_obj_tsv(os.path.join(cook2_IVD_dir, \"features/{}_obj36.tsv\".format(vid)))\n",
    "        \n",
    "        ## Make beign target  ( EX) [0, 0, 0, 1, ..... ,0, 1, 0, 0]\n",
    "        begin_target = []\n",
    "        end_target = []\n",
    "        \n",
    "        begin_distance = []\n",
    "        end_distance = []\n",
    "        \n",
    "        begin_ratio = []\n",
    "        end_ratio = []\n",
    "        \n",
    "        \n",
    "        mask = []\n",
    "        \n",
    "        for idx in range(max_frame_len):\n",
    "            __flag__ = 0\n",
    "            target_b = -1\n",
    "            target_e = -1\n",
    "            middle = -1\n",
    "            for b, e in segs_idx:\n",
    "                if(idx >= b and idx <= e): __flag__ = 1; target_b = b; target_e = e; middle = (b + e) / 2.0\n",
    "                    \n",
    "            \n",
    "            if(__flag__): \n",
    "                begin_target.append(1.); end_target.append(1.);\n",
    "                begin_distance.append(abs(target_b - idx)); end_distance.append(abs(target_e - idx)) \n",
    "                begin_ratio.append(torch.sigmoid(middle - idx)); end_ratio.append(torch.sigmoid(idx - middle))\n",
    "                \n",
    "                \n",
    "            else: \n",
    "                begin_target.append(0.); end_target.append(0.)\n",
    "                begin_distance.append(999.); end_distance.append(999.)\n",
    "                begin_ratio.append(0.); end_ratio.append(0.)\n",
    "                \n",
    "        \n",
    "            if(idx < len(frame_info)): \n",
    "                mask.append(True)\n",
    "            else: \n",
    "                mask.append(False)\n",
    "        \"\"\"\n",
    "        for idx in range(max_frame_len):\n",
    "            if(idx in begin_idxs): begin_target.append(1.)\n",
    "            else: begin_target.append(0.)\n",
    "                \n",
    "            if(idx in end_idxs): end_target.append(1.)\n",
    "            else: end_target.append(0.)\n",
    "                \n",
    "        \n",
    "            if(idx < len(frame_info)): mask.append(True)\n",
    "            else: mask.append(False)\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        return {\"vid\": vid, \"trans_idx\": trans_idx, \"trans\": trans, \"segs_idx\": segs_idx, \"segs\": segs, \"query\": query, \"frame_info\": frame_info, \\\n",
    "               \"begin_target\": begin_target, \"end_target\": end_target, \"mask\": mask,\\\n",
    "               \"begin_distance\": begin_distance, \"end_distance\": end_distance, \"begin_ratio\": begin_ratio, \"end_ratio\": end_ratio}\n",
    "    \n",
    "    \n",
    "def segment_collate(samples):\n",
    "    vid, query = [], []\n",
    "    trans_idx, trans = [], []\n",
    "    segs_idx, segs = [], []\n",
    "    frame_info = []\n",
    "    \n",
    "    begin_target, end_target = [], []\n",
    "    begin_distance, end_distance =  [], []\n",
    "    begin_ratio, end_ratio = [], []\n",
    "    mask = []\n",
    "    \n",
    "    max_trans_len = max( list(len(s[\"trans_idx\"]) for s in samples))\n",
    "    max_segs_len = max( list(len(s[\"segs_idx\"]) for s in samples))\n",
    "    #max_frame_len = max(list(len(s[\"frame_info\"]) for s in samples))\n",
    "    max_frame_len = 552\n",
    "\n",
    "    for sample in samples:\n",
    "        vid.append(sample[\"vid\"])\n",
    "        query.append(sample[\"query\"])\n",
    "        \n",
    "        trans_idx.append(sample[\"trans_idx\"])\n",
    "        trans.append(sample[\"trans\"] + [\"<PAD>\"] * (max_trans_len - len(sample[\"trans\"])) )\n",
    "        \n",
    "        segs_idx.append(sample[\"segs_idx\"])\n",
    "        segs.append(sample[\"segs\"] + [\"<PAD>\"] * (max_segs_len - len(sample[\"segs\"])) )\n",
    "        begin_target.append(sample[\"begin_target\"])\n",
    "        end_target.append(sample[\"end_target\"])\n",
    "        \n",
    "        begin_distance.append(sample[\"begin_distance\"]); end_distance.append(sample[\"end_distance\"])\n",
    "        begin_ratio.append(sample[\"begin_ratio\"]); end_ratio.append(sample[\"end_ratio\"])\n",
    "        mask.append(sample[\"mask\"])\n",
    "        \n",
    "        frame_info.append(sample[\"frame_info\"] + [\"<PAD>\"] * (max_frame_len - len(sample[\"frame_info\"])))\n",
    "        \n",
    "    padded_trans_idx = torch.nn.utils.rnn.pad_sequence(trans_idx, batch_first=True, padding_value = -4444).contiguous()\n",
    "    padded_trans = trans \n",
    "    padded_segs_idx = torch.nn.utils.rnn.pad_sequence(segs_idx, batch_first=True, padding_value = -4444).contiguous()\n",
    "    padded_segs = segs \n",
    "\n",
    "\n",
    "    return {\"vid\": vid, \"trans_idx\": padded_trans_idx, \"trans\": padded_trans, \\\n",
    "            \"segs_idx\": padded_segs_idx, \"segs\": padded_segs, \"query\": query, \"frame_info\": frame_info,\\\n",
    "           \"begin_target\": torch.tensor(begin_target), \"end_target\": torch.tensor(end_target), \"mask\": torch.tensor(mask),\\\n",
    "           \"begin_distance\": begin_distance, \"end_distance\": end_distance, \"begin_ratio\": begin_ratio, \"end_ratio\": end_ratio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "segment_dataset = segmentDataset(cook2_IVD_dir)\n",
    "valid_dataset = segmentDataset(cook2_IVD_dir, mode = 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(segment_dataset, collate_fn = segment_collate, batch_size = 1, shuffle = True, num_workers = 4 )\n",
    "validloader = DataLoader(valid_dataset, collate_fn = segment_collate, batch_size = 1, num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('yc2_recipes.json', 'r') as fp:\n",
    "    recipes = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1090/1090 [16:57<00:00,  1.07it/s]\n",
      "100%|██████████| 267/267 [04:26<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "    sampling_num = 3\n",
    "    matched_recipe = {}\n",
    "    with torch.no_grad():\n",
    "        image_encoder.eval()\n",
    "        lang_encoder.eval()\n",
    "        with autocast():\n",
    "            losses = 0\n",
    "            for i, batch in tqdm(enumerate(dataloader), total = len(dataloader)):\n",
    "                maxlen = max([len(i) for i in batch['frame_info']])\n",
    "                annot, segs_idx, item =batch['segs'][0],batch['segs_idx'][0],batch['frame_info'][0]\n",
    "                query = batch['query'][0]\n",
    "                vid = batch['vid'][0]\n",
    "                vid_recipe = recipes[query]\n",
    "                flatten_recipe = []\n",
    "                for recipe in vid_recipe:\n",
    "                    for recipe_doc in recipe['split_ins']:\n",
    "                        if recipe_doc!= '':\n",
    "                            flatten_recipe.append(recipe_doc)\n",
    "                frame_feats = []\n",
    "                box_feats = []\n",
    "                frame_indices = []\n",
    "                for frame_idx in range(len(item)//sampling_num):\n",
    "                    frame_idx = sampling_num*frame_idx+1\n",
    "                    if item[frame_idx] != '<PAD>':\n",
    "                        frame_feats.append(item[frame_idx]['features'])\n",
    "                        box_feats.append(item[frame_idx]['boxes'])\n",
    "                        frame_indices.append(frame_idx)\n",
    "                    else:\n",
    "                        break\n",
    "                visn_feats = torch.tensor(frame_feats).float().cuda(), torch.tensor(box_feats).float().cuda()\n",
    "                seq_output, pooled_output = image_encoder(visual_feats = visn_feats)\n",
    "                encode = [tokenizer.encode(sent) for sent in flatten_recipe]\n",
    "                maxlen = max([len(e) for e in encode])\n",
    "                encode = torch.tensor([e+[tokenizer.pad_token_id]*(maxlen-len(e)) for e in encode]).cuda()\n",
    "                output = lang_encoder(encode)\n",
    "                sequence_output, lang_pooled_output = output[0], output[1]\n",
    "                expanded_word = expansion_word(pooled_output, vid = vid)\n",
    "                concat1 = torch.cat(( pooled_output, expanded_word), dim = -1)\n",
    "                concat1 = merge1(concat1)\n",
    "                expanded_obj = expansion_obj(lang_pooled_output, vid = vid)\n",
    "                concat2 = torch.cat((lang_pooled_output, do_output2), dim = -1)\n",
    "                concat2 = merge2(concat2)\n",
    "                dotoutput = torch.matmul(concat1, concat2.transpose(1,0))\n",
    "                pred = np.argmax(-1*dotoutput.detach().cpu().numpy(), axis = -1)\n",
    "                vid_matched_recipes = {frame_indices[j]:flatten_recipe[pred[j]] for j in range(len(pred))}\n",
    "                matched_recipe[vid] = vid_matched_recipes\n",
    "            for i, batch in tqdm(enumerate(validloader), total = len(validloader)):\n",
    "                maxlen = max([len(i) for i in batch['frame_info']])\n",
    "                annot, segs_idx, item =batch['segs'][0],batch['segs_idx'][0],batch['frame_info'][0]\n",
    "                query = batch['query'][0]\n",
    "                vid = batch['vid'][0]\n",
    "                vid_recipe = recipes[query]\n",
    "                flatten_recipe = []\n",
    "                for recipe in vid_recipe:\n",
    "                    for recipe_doc in recipe['split_ins']:\n",
    "                        if recipe_doc!= '':\n",
    "                            flatten_recipe.append(recipe_doc)\n",
    "                frame_feats = []\n",
    "                box_feats = []\n",
    "                frame_indices = []\n",
    "                for frame_idx in range(len(item)//sampling_num):\n",
    "                    frame_idx = sampling_num*frame_idx+1\n",
    "                    if item[frame_idx] != '<PAD>':\n",
    "                        frame_feats.append(item[frame_idx]['features'])\n",
    "                        box_feats.append(item[frame_idx]['boxes'])\n",
    "                        frame_indices.append(frame_idx)\n",
    "                    else:\n",
    "                        break\n",
    "                visn_feats = torch.tensor(frame_feats).float().cuda(), torch.tensor(box_feats).float().cuda()\n",
    "                seq_output, pooled_output = image_encoder(visual_feats = visn_feats)\n",
    "                encode = [tokenizer.encode(sent) for sent in flatten_recipe]\n",
    "                maxlen = max([len(e) for e in encode])\n",
    "                encode = torch.tensor([e+[tokenizer.pad_token_id]*(maxlen-len(e)) for e in encode]).cuda()\n",
    "                output = lang_encoder(encode)\n",
    "                sequence_output, lang_pooled_output = output[0], output[1]\n",
    "                expanded_word = expansion_word(pooled_output, vid = vid)\n",
    "                concat1 = torch.cat(( pooled_output, expanded_word), dim = -1)\n",
    "                concat1 = merge1(concat1)\n",
    "                expanded_obj = expansion_obj(lang_pooled_output, vid = vid)\n",
    "                concat2 = torch.cat((lang_pooled_output, do_output2), dim = -1)\n",
    "                concat2 = merge2(concat2)\n",
    "                dotoutput = torch.matmul(concat1, concat2.transpose(1,0))\n",
    "                pred = np.argmax(-1*dotoutput.detach().cpu().numpy(), axis = -1)\n",
    "                vid_matched_recipes = {frame_indices[j]:flatten_recipe[pred[j]] for j in range(len(pred))}\n",
    "                matched_recipe[vid] = vid_matched_recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dual_encoder_final_matched_result.json', 'w') as fp:\n",
    "    json.dump(matched_recipe, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dual_encoder_final_matched_result.pkl', 'wb') as fp:\n",
    "    pkl.dump(matched_recipe, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
